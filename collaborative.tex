% !TeX root = main.tex
\chapter{Collaborative Manipulation}\label{ch:collaborative}

\section{Vector Field}
In this section, we revisit the vector field introduced in \cite{Rezende2022} and extend its applicability to include orientations, focusing exclusively on static curves. The vector field is designed to control the following first-order integrator model, where the control inputs are denoted as $\dot{\mathbf{p}}_d$ and $\boldsymbol{\omega}_d$:
\begin{align}
\begin{split}
    \dot{\mathbf{p}} &= \dot{\mathbf{p}}_d,\\
    \boldsymbol{\omega} &= \boldsymbol{\omega}_d.\label{eq:firstordersystemVF}
\end{split} 
\end{align}
Here $\mathbf{p}$ is the position, such that $\dot{\mathbf{p}}$ is the linear velocity, and $\boldsymbol{\omega}$ the angular velocity. 
The objective is to determine a vector field $\boldsymbol{\psi}\equiv\boldsymbol{\psi}(\mathbf{p}, \mathbf{R}):\mathbb{R}^3 \times \text{SO}(3)\to\mathbb{R}^6$, where $\mathbf{R}$ is a rotation matrix, such that if the twist $\boldsymbol{\xi} = \left[\dot{\mathbf{p}},\quad \boldsymbol{\omega} \right]= \left[\dot{\mathbf{p}}_d,\quad \boldsymbol{\omega}_d \right] = \boldsymbol{\psi}$, the system \eqref{eq:firstordersystemVF} follows the target curve $\mathcal{C} \subset \mathbb{R}^3\times \text{SO}(3)$, which incorporates an orientation frame attached to each of its points, represented by a rotation matrix.

To begin, we define an error function $\widehat{D}\equiv\widehat{D}(\mathbf{p}, \mathbf{R}, \mathbf{p}_d, \mathbf{R}_d)$ that measures the discrepancy between a pose $(\mathbf{p}, \mathbf{R})$ and a desired pose $(\mathbf{p}_d, \mathbf{R}_d)$. The error function is expressed as follows
\begin{equation}
    \widehat{D}(\mathbf{p}, \mathbf{R}, \mathbf{p}_d, \mathbf{R}_d) \equiv \frac{1}{2}\left(\|\mathbf{p} - \mathbf{p}_d\|^2 + \beta\left\|\mathbf{I} - \mathbf{R}_d^{T}\mathbf{R}\right\|^2_F\right),\! \label{eq:errorfuncDtilde}
\end{equation}
where $\beta$ is a scaling parameter in squared meters utilized to maintain dimensional consistency. The Frobenius norm measures the parallelism of the rotation matrices columns, allowing parameter $\beta$ to balance position and orientation errors.

Now, let's consider the parametric representation of the curve $C^2\ni\mathbf{h}(s):[0,1]\to\mathbb{R}^3\times \text{SO}(3)$, i.e., $\mathbf{h}(s)=(\mathbf{p}_d(s), \mathbf{R}_d(s))$, with following properties:
\begin{enumerate}[label=\roman*]
    \item $\mathbf{h}(s)$ is twice differentiable in $s$.
    \item $\mathbf{h}(s+1) = \mathbf{h}(s)\,\forall\, s$. That is, the curve is closed in the pose space.
    \item $s_1\neq s_2\implies \mathbf{h}(s_1)\neq\mathbf{h}(s_2),\,\forall\, s_1,s_2\in(0, 1)$. That is, the curve has no self-intersections.
    \item The pair $\Big(\frac{\partial \mathbf{p}_d}{\partial s}(s),  \frac{\partial \mathbf{R}_d}{\partial s}(s)\Big) \neq \mathbf{0}\, \forall\, s$. \label{prop:tangentenaonula}
\end{enumerate}
Next, define the curve's closest point to the system as follows:
\begin{align}
    s^*(\mathbf{p},\mathbf{R}) &\equiv \underset{s}{\arg\min}\;\widehat{D}\big(\mathbf{p}, \mathbf{R}, \mathbf{p}_d(s), \mathbf{R}_d(s)\big),\label{eq:otmproblemsStar}\\
    \mathbf{p}_d^*&\equiv\mathbf{p}_d^*(\mathbf{p},\mathbf{R}) = \mathbf{p}_d\left(s^*\left(\mathbf{p},\mathbf{R}\right)\right),\\
    \mathbf{R}_d^*&\equiv\mathbf{R}_d^*(\mathbf{p},\mathbf{R}) = \mathbf{R}_d\left(s^*\left(\mathbf{p},\mathbf{R}\right)\right).
\end{align}
% \underset{s}{\arg\min}\|\mathbf{p} - \mathbf{p}_d(s)\|^2 + \frac{\beta}{2}\left\|\mathbf{I} - \mathbf{R}_d^T(s)\mathbf{R}\right\|^2_F \label{eq:otmproblemsStar}\\
%     \mathbf{p}_d^*&\equiv\mathbf{p}_d^*(\mathbf{p},\mathbf{R}) = \mathbf{p}_d\left(s^*\left(\mathbf{p},\mathbf{R}\right)\right)\\
%     \mathbf{R}_d^*&\equiv\mathbf{R}_d^*(\mathbf{p},\mathbf{R}) = \mathbf{R}_d\left(s^*\left(\mathbf{p},\mathbf{R}\right)\right)

In \cite{Rezende2022}, a metric $D_a$ is defined in order to measure the distance from a point to a curve in $\mathbb{R}^3$. This metric is used to compute a normal vector that, if followed, guides the trajectory of the system to the curve. Hence, we introduce the metric $D\equiv D(\mathbf{p}, \mathbf{R}):\mathbb{R}^3\times \text{SO}(3)\to\mathbb{R}_+$ for the same purpose. This metric is defined as
\begin{equation}
    D(\mathbf{p}, \mathbf{R}) \equiv \widehat{D}\big(\mathbf{p}, \mathbf{R}, \mathbf{p}_d^*(\mathbf{p}, \mathbf{R}), \mathbf{R}_d^*(\mathbf{p}, \mathbf{R})\big).\label{eq:distancefunc}
\end{equation}
This metric clearly quantifies the closeness of the point to the target curve.
% \|\mathbf{p} - \mathbf{p}_d^*\|^2 + \frac{\beta}{2}\left\|\mathbf{I} - \mathbf{R}_d^{*T}\mathbf{R}\right\|^2_F

Moreover, in \cite{Rezende2022}, alongside the normal vector $\boldsymbol{\xi}_{N_a}$, a tangent vector $\boldsymbol{\xi}_{T_a}$ is introduced. While the normal component guides the system trajectories onto the desired curve, the tangent component ensures that the trajectories circulate along the curve. We now introduce our normal vector $\boldsymbol{\xi}_N$, which also incorporates orientation.

Let $\mathbf{R}=\left[\mathbf{u}_x\,\mathbf{u}_y\,\mathbf{u}_z\right]$, $\mathbf{R}^*_d=\left[\mathbf{u}^*_{xd}\,\mathbf{u}^*_{yd}\,\mathbf{u}^*_{zd}\right]$. Define $\boldsymbol{\sigma}\equiv\boldsymbol{\sigma}(\mathbf{p}, \mathbf{R})$ as
% In \cite{Rezende2022}, we have a convergent $\boldsymbol{\xi}_N$ and tangent $\boldsymbol{\xi}_T$ component. The convergent component is responsible for guiding a point to the curve, and the tangent component makes it circulate the curve. We start with the convergent vector. Let $\mathbf{R}=\left[\mathbf{u}_x\,\mathbf{u}_y\,\mathbf{u}_z\right]$, $\mathbf{R}^*_d=\left[\mathbf{u}^*_{xd}\,\mathbf{u}^*_{yd}\,\mathbf{u}^*_{zd}\right]$. Define $\boldsymbol{\sigma}\equiv\boldsymbol{\sigma}(\mathbf{p}, \mathbf{R})$ as
\begin{align}
\begin{split}
    \boldsymbol{\sigma}&\equiv\beta\!\!\left.\left(\mathbf{u}_x\times \frac{\partial \widehat{D}}{\partial \mathbf{u}_x}^T\!\!\!\! + \mathbf{u}_y\times \frac{\partial \widehat{D}}{\partial \mathbf{u}_y}^T\!\!\!\! + \mathbf{u}_z\times \frac{\partial \widehat{D}}{\partial \mathbf{u}_z}^T\right)\right|_{\mathbf{p}_d^*, \mathbf{R}_d^*}\\
    &= \beta\left(\mathbf{u}^*_{xd}\times\mathbf{u}_x + \mathbf{u}^*_{yd}\times\mathbf{u}_y + \mathbf{u}^*_{zd}\times\mathbf{u}_z\right).
\end{split}
\end{align}
With this definition of $\boldsymbol{\sigma}$, we then define the normal vector $\boldsymbol{\xi}_N\equiv\boldsymbol{\xi}_N(\mathbf{p}, \mathbf{R})$ as
\begin{equation}
     \boldsymbol{\xi}_N(\mathbf{p}, \mathbf{R}) \equiv \begin{bmatrix}
         \frac{\partial \widehat{D}}{\partial \mathbf{p}}^T\Big|_{\mathbf{p}_d^*, \mathbf{R}_d^*}\\ \boldsymbol{\sigma}
     \end{bmatrix} 
     = \begin{bmatrix}
         \mathbf{p} - \mathbf{p}_d^*\\ \boldsymbol{\sigma}
     \end{bmatrix}.
\end{equation}
A significant feature of the normal vector $\boldsymbol{\xi}_{N_a}$ introduced in \cite{Rezende2022} is its association with their distance function $D_a$. It was demonstrated that $\dot{D}_a = \frac{\partial D_a}{\partial \mathbf{p}}\dot{\mathbf{p}} = \boldsymbol{\xi}_{N_a}^T\dot{\mathbf{p}}$. Similarly, for our case, applying the chain rule reveals that
% In \cite{Rezende2022} its in shown a key feature of their convergent vector $\boldsymbol{\xi}_N_a$ with relationship with their distance $D_a$, which is $\dot{D}_a = \frac{\partial D_a}{\partial \mathbf{p}}\dot{\mathbf{p}} = \boldsymbol{\xi}_N_a^T\dot{\mathbf{p}}$. Similarly, it can be seen by the chain rule that
\begin{align}
\begin{split}
     \dot{D} = &\frac{\partial \widehat{D}}{\partial \mathbf{p}}\dot{\mathbf{p}} + \frac{\partial \widehat{D}}{\partial \mathbf{u}_x}(\boldsymbol{\omega}\times \mathbf{u}_x) + \frac{\partial \widehat{D}}{\partial \mathbf{u}_y}(\boldsymbol{\omega}\times \mathbf{u}_y)+ \frac{\partial \widehat{D}}{\partial \mathbf{u}_z}(\boldsymbol{\omega}\times \mathbf{u}_z) \\
     &+ \left\{ \frac{\partial \widehat{D}}{\partial \mathbf{p}_d}\left.\frac{\partial \mathbf{p}_d}{\partial s}\right|_{s^*}\!\!\!\! + \frac{\partial \widehat{D}}{\partial \mathbf{u}_{xd}}\left.\frac{\partial \mathbf{u}_{xd}}{\partial s}\right|_{s^*}\!\!\!\!
     + \frac{\partial \widehat{D}}{\partial \mathbf{u}_{yd}}\left.\frac{\partial \mathbf{u}_{yd}}{\partial s}\right|_{s^*}\!\!\!\! 
     + \frac{\partial \widehat{D}}{\partial \mathbf{u}_{zd}}\left.\frac{\partial \mathbf{u}_{zd}}{\partial s}\right|_{s^*}\right\}\dot{s}^{*}.
\end{split}
\end{align}
Since $s^*$ is the solution to the optimization problem \eqref{eq:otmproblemsStar}, the first-order optimality condition implies that the term within braces is zero. Furthermore, utilizing properties of the cross product, we deduce that
\begin{equation}
    \dot{D} = \boldsymbol{\xi}_N^T\boldsymbol{\xi}, \label{eq:keyfeaturenormal}
\end{equation}
which mirrors the aforementioned feature.

In order to define the tangent component, we acknowledge that the tangent space of the Lie group $\text{SO}(3)$ corresponds to the Lie algebra $\mathfrak{so}(3)$. Therefore, the angular velocity associated with the tangent component is represented by the vector corresponding to a skew-symmetric matrix:
\begin{align}
    % \begin{split}
        \frac{\partial \mathbf{R}_d}{\partial s} = \SL[\boldsymbol{\omega}_d(s)]\mathbf{R}_d(s)
        \implies \boldsymbol{\omega}_d(s) = \invSL[\frac{\partial \mathbf{R}_d}{\partial s}\mathbf{R}_d^T(s)].
    % \end{split}
\end{align}
With this, we finally define the curve tangent vector $\boldsymbol{\xi}_T$, in accordance with \cref{prop:tangentenaonula}, as
\begin{equation}
    \boldsymbol{\xi}_T(\mathbf{p}, \mathbf{R}) \equiv \begin{bmatrix}
        \frac{\partial \mathbf{p}_d}{\partial s}(s^*(\mathbf{p}, \mathbf{R}))\vspace{1mm}\\
        \boldsymbol{\omega}_d(s^*(\mathbf{p}, \mathbf{R}))
    \end{bmatrix}.
\end{equation}
Another noteworthy feature present in \cite{Rezende2022} is the orthogonality between their normal and tangent vectors. In our case, this holds true as well:
% In \cite{Rezende2022}, a key feature of their tangent vector is the orthogonality with $\boldsymbol{\xi}_N$. In our case that is also true:
\begin{align}
\begin{split}
\boldsymbol{\xi}_N^T\boldsymbol{\xi}_T = (\mathbf{p} - \mathbf{p}_d)^T\frac{\partial \mathbf{p}_d}{\partial s} + \boldsymbol{\sigma}^T\boldsymbol{\omega}_d = 0,
\end{split} \label{eq:orthogonality}
\end{align}
% \frac{\partial D}{\partial \mathbf{p}}\frac{\partial \mathbf{p}_d}{\partial s}(s^*) + 
%    \mathbf{u}_x^T\skewS{\boldsymbol{\omega}_d(s)}\mathbf{u}^*_{xd} \\&+ \mathbf{u}_y^T\skewS{\boldsymbol{\omega}_d(s)}\mathbf{u}^*_{yd} + \mathbf{u}_z^T\skewS{\boldsymbol{\omega}_d(s)}\mathbf{u}^*_{zd}    \\
%    =& \frac{\partial D}{\partial \mathbf{p}}\frac{\partial \mathbf{p}_d}{\partial s}(s^*)
%    + \frac{\partial D}{\partial \mathbf{u}_{xd}}\left.\frac{\partial \mathbf{u}_{xd}}{\partial s}\right|_{s^*}\\
%    &+\frac{\partial D}{\partial \mathbf{u}_{yd}}\left.\frac{\partial \mathbf{u}_{yd}}{\partial s}\right|_{s^*}\!\!\!\!
%    + \frac{\partial D}{\partial \mathbf{u}_{zd}}\left.\frac{\partial \mathbf{u}_{zd}}{\partial s}\right|_{s^*}
due to the first-order optimality condition of the optimization problem \eqref{eq:otmproblemsStar} and utilizing properties of the vector product.

Two other definitions are also required to define the vector field: a differentiable strictly increasing function $G\equiv G(D):\mathbb{R}_+\to\mathbb{R}_+$, with $G(D)=0\iff D=0$; and a differentiable positive function $H\equiv H(D):\mathbb{R}_+\to\mathbb{R}_+$. With these definitions, we define our vector field as
% Finally, we define the following functions $G$ and $H$ as:
% $G$ is a continuous increasing function and $H$ is a positive continuous function. Thus, we define the vector field as
% \begin{align}
%     G\equiv G(D)&=\frac{2}{\pi}\arctan(k_GD) \label{eq:G}\\
%     H\equiv H(D) &= \sqrt{1 - G(D)^2} \label{eq:H}.
% \end{align}
% Here, $k_G>0$ represents a gain parameter that determines the convergence weight.
\begin{equation}
    \boldsymbol{\psi} = -G\boldsymbol{\xi}_N +H\boldsymbol{\xi}_T. \label{eq:vectorfield}
\end{equation}
Next, we prove that the trajectories of the system $\boldsymbol{\xi}=\boldsymbol{\psi}$ converge to $\mathcal{C}$ using Lyapunov stability theory. Let $V=D$ be a Lyapunov function candidate, then its time derivative is given by
\begin{align}
    \dot{V} = \boldsymbol{\xi}_N^T\boldsymbol{\xi} = -G\|\boldsymbol{\xi}_N\|^2 + H\boldsymbol{\xi}_N^T\boldsymbol{\xi}_T = -G\|\boldsymbol{\xi}_N\|^2\le0,
\end{align}
where we used the feature \eqref{eq:keyfeaturenormal} and the orthogonality property \eqref{eq:orthogonality}.

Now, we examine where $\dot{V}$ equals zero. According to its definition, $G$ equals zero only on the target curve. Breaking up $\|\boldsymbol{\xi}_N\|^2 = \|\mathbf{p} - \mathbf{p}_d\|^2 + \| \boldsymbol{\sigma}\|^2$, we observe that the first term is zero only if $\|\mathbf{p} - \mathbf{p}_d\|^2$, but $\|\boldsymbol{\sigma}\|^2$ can be zero if $\text{tr}(\mathbf{R}_d^{*T}\mathbf{R})=3$ or $\text{tr}(\mathbf{R}_d^{*T}\mathbf{R})=-1$ \citep{Culbertson2021}. The first case implies that the position and orientation are exactly the nearest ones on the curve. As for the second case, the orientation is not the desired one. For instance, if $\mathbf{u}_x = -\mathbf{u}_{xd}^*, \mathbf{u}_y = -\mathbf{u}_{yd}^*\implies \mathbf{u}_z = \mathbf{u}_{zd}^*$, then $\boldsymbol{\sigma}=0$ but $\mathbf{R}_d^{*T}\mathbf{R}\neq \mathbf{I}$. 

Similarly as in Theorem 1 in \cite{Culbertson2021}, this spurious equilibrium point is a local maximum value of the Lyapunov function. As such, it signifies an unstable equilibrium point, indicating that any perturbation will guide the orientation to the desired one. Consequently, we conclude that the trajectories almost globally asymptotically converge to $\mathcal{C}$. Note that, when on $\mathcal{C}$, circulation is ensured if the tangent component follows \cref{prop:tangentenaonula}.
% We need now to analyze where the Lyapunov function derivative is zero. $G$ is only zero in the target curve, whereas $\|\boldsymbol{\xi}_N\|^2 = \|\mathbf{p} - \mathbf{p}_d\|^2 + \| \boldsymbol{\sigma}\|^2$ is zero in the target curve, but can be zero outside as well. More precisely, this is true when $\mathbf{p}=\mathbf{p}_d$, but for the orientation part, it can be zero when $\text{tr}(\mathbf{R}_d^{*T}\mathbf{R})=3$, i.e. $\mathbf{R}^{*T}\mathbf{R}=\mathbf{I}$, or $\text{tr}(\mathbf{R}_d^{*T}\mathbf{R})=-1$ \citep{Culbertson2021}. From \cite{Culbertson2021}, it is known that a singularity occurs for $\|\sigma\|$ when $\text{tr}(\mathbf{R}^{*T}\mathbf{R})=-1$. For instance, if $\mathbf{x} = -\mathbf{x}_{d}^*, \mathbf{y} = -\mathbf{y}_{d}^*\implies \mathbf{z} = \mathbf{z}_{d}^*$, then $\sigma=0$ but $\mathbf{R}_d^{*T}\mathbf{R}\neq \mathbf{I}$. This point is an unstable equilibrium point. Similarly as in Thoerem 1 in \cite{Culbertson2021}, this spurious equilibrium point is the maximum value of the Lyapunov function, this implies that this point is an unstable equilibrium point, thus any perturbation will guide the current orientation to the desired one.

\vspace{-1mm}
\section{Collaborative Manipulation}
\vspace{-1mm}

% \textbf{MUDAR O COMEÇO, ESTA DEPENDENTE DA SECAO QUE VEM DEPOIS}
The problem addressed in this section revolves around designing an adaptive controller to guide a manipulated object along a predefined curve $\mathcal{C} \subset \mathbb{R}^3\times \text{SO}(3)$. The object's dynamics include uncertain parameters like mass and geometric properties. With $N$ agents involved in the manipulation process, a decentralized control law is devised for each agent. The agents are unaware of their precise positioning relative to the measurement point. It is assumed that each agent possesses the capability to measure the pose and velocity of the object's measurement point.
% The problem addressed in this work revolves around designing an adaptive controller to guide a manipulated object along a predefined curve. The dynamics of the object, expressed by equations \eqref{eq:tau} and \eqref{eq:tauNtauIrelation}, incorporate uncertain parameters such as mass properties $m$ and $\mathbb{I}_p^\mathfrak{B}$, as well as geometric properties $\mathbf{r}_p^\mathfrak{B}$ and $\mathbf{r}_i$. With $N$ agents involved in the manipulation process, a decentralized control law is devised for each agent. It is assumed that each agent possesses the capability to measure the pose and velocity of the object's measurement point.
% The curve $\mathcal{C} \subset \mathbb{R}^3\times \text{SO}(3)$ incorporates an orientation frame attached to each of its points, represented by a rotation matrix.

\vspace{-1mm}
\subsection{System Modeling}
\vspace{-1mm}
\begin{figure}[ht]
    \centering
    \def\svgwidth{.8\linewidth}
    \import{figures/}{collaborative_scheme.pdf_tex}
    \caption{Cooperative task}
    \label{fig:problem}
\end{figure}
We consider the cooperative manipulation of a rigid body by a team of $N$ autonomous agents, as illustrated in \cref{fig:problem}. The body undergoes both translations and rotations. We establish two reference frames: the world-fixed inertial frame denoted $\mathfrak{I}$, and the body-fixed frame denoted $\mathfrak{B}$, centered at the body's center of mass, $\mathbf{b}_\text{cm}^\mathfrak{B}$. Additionally, a body-fixed measurement point, $\mathbf{p}^\mathfrak{B}$, is situated at a distance $\mathbf{r}_p^\mathfrak{B}$ from the body's center of mass, $\mathbf{b}_\text{cm}^\mathfrak{B}$.

The body possesses a mass $m$ and a constant inertia tensor $\mathbb{I}_\text{cm}^\mathfrak{B}$ about its center of mass. We assume each agent is rigidly attached to the body at $\mathbf{r}_i$ from $\mathbf{p}^\mathfrak{B}$. While $\mathbf{r}_i$ ideally should be expressed relative to a frame at $\mathbf{p}^\mathfrak{B}$, for simplicity, we interchangeably consider $\mathbf{r}_i\equiv \mathbf{r}_i^\mathfrak{B}$. This assumption is made under the understanding that this information solely pertains to torque computations, and the frame associated with the measurement point shares the same orientation as that of the center of mass. Each agent exerts a wrench $\boldsymbol{\tau}_i$ onto the body.

Define the pose $\mathbf{q} = (\mathbf{p}, \mathbf{R}) \in \mathbb{R}^3\times \text{SO}(3)$, where $\mathbf{p}$ is the position of the measurement point in $\mathfrak{I}$ and $\mathbf{R}\equiv \mathbf{R}_\mathfrak{B}^\mathfrak{I}$ is the rotation matrix from $\mathfrak{B}$ to $\mathfrak{I}$. Abusing notation, let $\mathbb{R}^6\ni\dot{\mathbf{q}} = \left[\dot{\mathbf{p}}^T, \boldsymbol{\omega}^T \right]^T$ represent the measurement point's linear and angular velocities, and $\mathbb{R}^6\ni\ddot{\mathbf{q}} = \left[\ddot{\mathbf{p}}^T, \boldsymbol{\alpha}^T \right]^T$ represent its linear and angular accelerations. With this notation, the dynamics can be succinctly expressed as \citep{Culbertson2021}
\begin{equation}
    \boldsymbol{\tau } = \mathbf {M}(\mathbf {q})\ddot{\mathbf {q}} + \mathbf {C}(\mathbf {q},\dot{\mathbf {q}})\dot{\mathbf {q}} \label{eq:tau}
\end{equation}
where $\boldsymbol{\tau }$ denotes the total wrench applied to the body about the point $\mathbf{p}$. The system's Cartesian space inertia matrix is represented by
\begin{equation}
    \mathbf {M}(\mathbf {q}) = \left[\begin{array}{cc}m\mathbf {I} & m\SL[\mathbf {R} \mathbf {r}_p^\mathfrak{B}] \\ -m\SL[\mathbf {R} \mathbf {r}_p^\mathfrak{B}] & \mathbf {R} \mathbb{I}_p^\mathfrak{B} \mathbf {R}^T \end{array} \right],\label{eq:Mcartesian}
\end{equation}
whereas the matrix $\mathbf {C}(\mathbf {q},\dot{\mathbf {q}})$ encompasses Coriolis and centrifugal terms:
\begin{equation}
\resizebox{0.88\columnwidth}{!}{$%
    \mathbf {C}(\mathbf {q},\dot{\mathbf {q}}) = \left[\begin{array}{cc}\boldsymbol{0}_{3\times3} & m\SL[\boldsymbol{\omega }^\mathfrak{I}] \SL[\mathbf {R} \mathbf {r}_p^\mathfrak{B}] \\ -m\SL[\boldsymbol{\omega }^\mathfrak{I}]\SL[\mathbf {R} \mathbf {r}_p^\mathfrak{B}] & \quad\SL[\boldsymbol{\omega }^\mathfrak{I}] \mathbf {R}\mathbb{I}_p^\mathfrak{B}\mathbf {R}^T 
    -m\SL[\SL[\mathbf {R}\mathbf {r}_p^\mathfrak{B}] \dot{\mathbf {x}}] \end{array} \right]. \label{eq:C}
$%
}
\end{equation}
The inertia tensor \(\mathbb{I}_p^\mathfrak{B}\) is determined by applying Steiner's theorem \citep{lemos2018analytical}: \(\mathbb{I}_p^\mathfrak{B} = \mathbb{I}_\text{cm}^\mathfrak{B} - m\SL[\mathbf{r}_p^\mathfrak{B}]^2\).

The dynamics described in \eqref{eq:tau} govern the motion of the body under the influence of the total wrench $\boldsymbol{\tau}$ applied about $\mathbf{p}$. This quantity needs to be expressed as a function of the wrenches $\boldsymbol{\tau}_i$ applied by each agent about their respective displacement $\mathbf{r}_i$ from $\mathbf{p}$. We represent $\boldsymbol{\tau}$ as the sum:
\begin{equation}
    \boldsymbol{\tau} = \sum_{i=1}^N\mathbf{G}(\mathbf{q}, \mathbf{r}_i)\boldsymbol{\tau}_i, \label{eq:tauNtauIrelation}
\end{equation}
where $\mathbf{G}$ denotes the grasp matrix, defined as:
\begin{equation}
    \mathbf{G}(\mathbf{q}, \mathbf{r}_i) = \begin{bmatrix}
        \mathbf{I}_{3\times3} & \mathbf{0}_{3\times3} \\
        \SL[\mathbf{R}\mathbf{r}_i] & \mathbf{I}_{3\times3} 
    \end{bmatrix}. \label{eq:graspmatrix}
\end{equation}

\vspace{-1mm}
\subsection{Decentralized Adaptive Control}
\vspace{-1mm}

In this section, we modify the adaptive control strategy proposed by \cite{Culbertson2021}. In their approach, an error vector is established between the system velocity and a velocity reference, resulting in a non-autonomous system. In contrast, our method defines the velocity error $\boldsymbol{\zeta}$ with respect to the vector field, thereby establishing an autonomous system. The vector is designed in such a way that when $\boldsymbol{\zeta}=0$, the system converges almost globally asymptotically to the curve $\mathcal{C}$. After this modification, the subsequent analysis proceeds similarly to the original approach. This error vector is expressed as:
% We define a velocity error vector $\mathbf{e}$ between the system velocities and the vector field. The vector is designed in such a way that when $\mathbf{e}=0$, the system converges almost globally asymptotically to and follows the curve $\mathcal{C}$. 
% The proposed adaptive control strategy draws inspiration from \cite{Culbertson2021} and aims to achieve similar outcomes using vector fields. We commence by defining an error vector $\mathbf{s}$ such that when $\mathbf{s}=0$, the system converges almost globally asymptotically to and follows the curve $\mathcal{C}$. This error vector is expressed as:
\begin{equation}
    \boldsymbol{\zeta} = \dot{\mathbf{q}} - \boldsymbol{\psi},\label{eq:errorvector-s}
\end{equation}
where $\mathbf{q}$ denotes the pose of the measurement point, and $\dot{\mathbf{q}}$ represents its corresponding twist.

It's noteworthy that when $\boldsymbol{\zeta}=0$, the system follows the same integrator model \eqref{eq:firstordersystemVF}, with the vector field $\boldsymbol{\psi}$ acting as its input. Therefore, leveraging the insights from the previous section, we anticipate that the system will exhibit the desired behavior.

Moving forward, we introduce a reference model. Given that the system is overactuated (meaning multiple sets of control inputs $\boldsymbol{\tau}_i$ produce identical object dynamics), each agent can contribute a portion of the control effort. Thus, we define a set of $N$ positive constants $\alpha_i$, ensuring $\sum_{i=1}^N\alpha_i=1$. The reference model is then characterized by:
\begin{equation}
    \alpha _i \left(\mathbf {M}(\mathbf {q})\dot{\boldsymbol{\psi}} + \mathbf {C}(\mathbf {q},\dot{\mathbf {q}})\boldsymbol{\psi} + \mathbf {g} \right) = \boldsymbol{\tau}_i.
\end{equation}
Note that $\dot{\boldsymbol{\psi}}$ can be numerically approximated. Furthermore, exploiting the linearity in the parameters within the equations of motion \citep{spong2020robot}, we can represent the object's dynamics using a regressor matrix $\mathbf{Y}_o$ and a parameter vector $\mathbf{o}_i$. Here, $\mathbf{o}_i = \alpha_i\mathbf{o}$ denotes a vector of the object's physical parameters, $\mathbf{o}$, such as mass or moments of inertia, scaled by $\alpha_i$:
\begin{equation}
    \alpha _i \left(\mathbf {M}(\mathbf {q})\dot{\boldsymbol{\psi}} + \mathbf {C}(\mathbf {q},\dot{\mathbf {q}})\boldsymbol{\psi} + \mathbf {g} \right) = \mathbf {Y}_o\left(\mathbf{q},\dot{\mathbf{q}},\boldsymbol{\psi}, \dot{\boldsymbol{\psi}}\right) \mathbf {o}_i. \label{eq:linearparametrizationModelRef}
\end{equation}
Summing both sides of the equation from 1 to N yields the same dynamics as in equation \eqref{eq:tau} with an additional gravity term. Furthermore, since the agents estimate the scaled vector $\mathbf{o}_i$, the constants $\alpha_i$ need not be known a priori.

Next, we propose the following control law
\begin{equation}
    \boldsymbol{\tau}_i = \mathbf{G}(\hat{\mathbf{r}}_i)^{-1}\boldsymbol{\eta}_i,\label{eq:controlLawTaui}
\end{equation}
where $\hat{\mathbf{r}}_i$ represents the estimate of agent $i$'s displacement with respect to the measurement point, and
\begin{equation}
    \boldsymbol{\eta}_i = \mathbf{Y}_o\hat{\mathbf{o}}_i - \mathbf{K}_D\boldsymbol{\zeta}, \label{eq:controlLawEtai}
\end{equation}
with $\mathbf{K}_D$ being a positive definite matrix.

It is essential to note that:
\begin{equation}
    \mathbf{G}_i\mathbf{G}(\hat{\mathbf{r}}_i)^{-1} = \mathbf{I} - \mathbf{G}(\widetilde{\mathbf{r}}_i), \label{eq:relationGiGiInvtoProof}
\end{equation}
where $\widetilde{\mathbf{r}}_i = \hat{\mathbf{r}}_i - \mathbf{r}_i$.

Furthermore, we define the following linear parametrization:
\begin{equation}
    -\mathbf{G}(\widetilde{\mathbf{r}}_i)\boldsymbol{\eta}_i = \mathbf{Y}_r\left(\boldsymbol{\eta}_i, \mathbf{q}\right)\widetilde{\mathbf{r}}_i. \label{eq:linearParametrizationYrforEta}
\end{equation}
Using the provided expressions, we propose the following adaptation laws
\begin{align}
    \dot{\hat{\mathbf{o}}}_i &= -\boldsymbol{\Gamma}_o\mathbf{Y}^T_o\left(\mathbf{q},\dot{\mathbf{q}},\boldsymbol{\psi}, \dot{\boldsymbol{\psi}}\right)\boldsymbol{\zeta},  \label{eq:adaptiveOhat}\\
    \dot{\hat{\mathbf{r}}}_i &= -\boldsymbol{\Gamma}_r\mathbf{Y}^T_r\left(\boldsymbol{\eta}_i, \mathbf{q}\right)\boldsymbol{\zeta},  \label{eq:adaptiveRhat}
\end{align}
where $\hat{\mathbf{o}}_i$ represents agent $i$'s estimate of the system parameters. $\boldsymbol{\Gamma}_o$ and $\boldsymbol{\Gamma}_r$ are definite positive matrices corresponding to the adaptation gains.
\begin{theorem}
    Consider the control laws \eqref{eq:controlLawTaui} and \eqref{eq:controlLawEtai}, and the adaptation laws \eqref{eq:adaptiveOhat} and \eqref{eq:adaptiveRhat}. Under the proposed adaptive controller, all control signals remain bounded, and the system converges to and follows the curve $\mathcal{C}$ from almost all initial conditions.
\end{theorem}
\begin{proof}
    Consider the following Lyapunov function candidate
    \begin{equation}
        V = \frac{1}{2}\left( \boldsymbol{\zeta}^T \mathbf {M} \boldsymbol{\zeta} + \sum _{i=1}^{N} \widetilde{\mathbf{o}}_i^{\,T} \boldsymbol{\Gamma }_o^{-1}\widetilde{\mathbf{o}}_i + \widetilde{\mathbf{r}}_i^{\,T} \boldsymbol{\Gamma }_r^{-1} \widetilde{\mathbf{r}}_i \right),
    \end{equation}
    where $\widetilde{\mathbf {o}}_i = \hat{\mathbf{o}}_i - \mathbf{o}_i$ represents the parameter estimation error for agent $i$.

    Since the true parameters do not vary in time, i.e. $\dot{\widetilde{\mathbf {o}}}_i = \dot{\hat{\mathbf{o}}}_i$ and $\dot{\widetilde{\mathbf {r}}}_i = \dot{\hat{\mathbf{r}}}_i$, the time derivative of the Lyapunov candidate yields
    \begin{equation}
        \dot{V} = \boldsymbol{\zeta}^T \mathbf{M} \dot{\boldsymbol{\zeta}} + \frac{1}{2} \boldsymbol{\zeta}^T \dot{\mathbf{M}} \boldsymbol{\zeta} + \sum\limits_{i=1}^{N} \widetilde{\mathbf{o}}_i^{\,T} \boldsymbol{\Gamma}_o^{-1} \dot{\hat{\mathbf{o}}}_i + \widetilde{\mathbf{r}}_i^{\,T} \boldsymbol{\Gamma}_r^{-1}\dot{\hat{\mathbf{r}}}_i.
    \end{equation}
    Using the fact that $\dot{\boldsymbol{\zeta}} = \ddot{\mathbf{q}} - \dot{\boldsymbol{\psi}}$ and $\dot{\mathbf{q}} = \boldsymbol{\zeta} + \boldsymbol{\psi}$, along with the system dynamics equations, we can expand the term
    \begin{align} 
    \begin{split}
        \boldsymbol{\zeta}^T \mathbf {M} \dot{\boldsymbol{\zeta}} &= \boldsymbol{\zeta}^T\mathbf{M}(\ddot{\mathbf{q}} - \dot{\boldsymbol{\psi}})\\ 
        &= \boldsymbol{\zeta}^T\left(\textstyle \sum\limits_{i=1}^{N} \mathbf{G}_i\boldsymbol{\tau }_i - \mathbf{C}\boldsymbol{\psi} - \mathbf{C} \boldsymbol{\zeta} - \mathbf{g} - \mathbf{M}\dot{\boldsymbol{\psi}}\right).
    \end{split}
    \end{align} 
    Thus, using \eqref{eq:linearparametrizationModelRef}, \eqref{eq:controlLawTaui}, \eqref{eq:relationGiGiInvtoProof} and \eqref{eq:linearParametrizationYrforEta}, we find
    \begin{align}
    \begin{split}
        \dot{V} &= \sum\limits_{i=1}^{N} \boldsymbol{\zeta}^T\left(\boldsymbol{\eta}_i + \mathbf {Y}_r \widetilde{\mathbf {r}}_i - \mathbf {Y}_o \mathbf {o}_i\right) + \frac{1}{2}\boldsymbol{\zeta}^T\left(\dot{\mathbf {M}}-2\mathbf {C}\right)\boldsymbol{\zeta} \!\\ &\quad \quad + \widetilde{\mathbf {o}}_i^{\,T} \boldsymbol{\Gamma }_o^{-1} \dot{\hat{\mathbf {o}}}_i + \widetilde{\mathbf {r}}_i^{\,T} \boldsymbol{\Gamma }_r^{-1}\dot{\hat{\mathbf {r}}}_i. 
        \end{split}
    \end{align}
    Noting that $\dot{\mathbf{M}} - 2\mathbf{C}$ is skew-symmetric, and applying \eqref{eq:controlLawEtai} while substituting adaptation laws \eqref{eq:adaptiveOhat} and \eqref{eq:adaptiveRhat}, we have:
    \begin{align}
       \hspace{-2mm} \dot{V} &{=} \sum\limits_{i=1}^{N} \boldsymbol{\zeta}^T\!\!\left({-}\mathbf{K}_D\boldsymbol{\zeta} {+} \mathbf {Y}_r \widetilde{\mathbf {r}}_i {+}\mathbf{Y}_o \widetilde{\mathbf{o}}_i\right)  {-} \widetilde{\mathbf {o}}_i^{\,T}\mathbf{Y}_o^T\boldsymbol{\zeta} {-} \widetilde{\mathbf {r}}_i^{\,T}\mathbf{Y}_r^T\boldsymbol{\zeta}. 
    \end{align}
    Note that all terms are scalars, hence we can transpose them, which results in
    \begin{equation}
        \dot{V} = -N\boldsymbol{\zeta}^T\mathbf{K}_D\boldsymbol{\zeta} \le 0.
    \end{equation}
    Thus, $\boldsymbol{\zeta}\to0$ as $t\to\infty$, and almost all trajectories converge asymptotically to $\mathcal{C}$. The curve circulation is ensured by the tangent component. 
    % To show this, we invoke Barbalat's Lemma \citep[p. 323]{khalil2002nonlinear} and prove that $\dot{V}$ is uniformly continuous. First, note that since $\dot{V}\le0$ and $V$ is lower bounded, this implies that $\mathbf{s}, \widetilde{\mathbf {o}}_i, \widetilde{\mathbf {r}}_i$ are bounded for all $i$. $\ddot{V}$ is given by:
    % % \begin{equation}
    %     \ddot{V} = -N\mathbf{s}^T\mathbf{K}_D\dot{\mathbf{s}}.
    % \end{equation}
    % Then, if $\dot{\mathbf{s}}$ is bounded, the proof is complete. Note that we can write the following
    % \begin{equation}
    %     \mathbf{M}\dot{\mathbf{s}} = \mathbf{M}(\ddot{\mathbf{q}} - \dot{\boldsymbol{\psi}}) = \sum_{i=1}^{N} (\mathbf{Y}_o \widetilde{\mathbf{o}}_i + \mathbf{Y}_r \widetilde{\mathbf{r}}_i) - (N\mathbf{K}_D + \mathbf{C})\mathbf{s}.
    % \end{equation}
    % The vector field and its derivative are inherently bounded due to their construction. Furthermore, as previously demonstrated, the remaining terms are also bounded. Given that $\mathbf{M}$ is definitively positive, its inverse $\mathbf{M}^{-1}$ exists, ensuring the boundedness of $\dot{\mathbf{s}}$.

    % This demonstrates that $\ddot{V}$ is bounded, implying that $\dot{V}$ is uniformly continuous, and consequently, $\mathbf{s}\to0$ as $t\to\infty$. Therefore, if the system does not initiate at the singularity where $\text{tr}\left(\mathbf{R}_d^{*T}\mathbf{R}\right)=-1$, then $\boldsymbol{\xi}-\boldsymbol{\psi}$ and $\mathbf{p}-\mathbf{p}^*$ approach zero, and $\mathbf{R}_d^{*T}\mathbf{R}$ tends to $\mathbf{I}$, ensuring the system's convergence to the curve $\mathcal{C}$. The system's trajectory follows the curve due to the tangent component.
\end{proof}