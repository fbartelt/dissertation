% !TeX root = main.tex
\chapter{Kinematic Control}\label{ch:kinematic}
The proposed vector field strategy was developed for any connected matrix Lie group. In this section, we define a path and EE-distance function specifically for a class of groups known as \emph{exponential} Lie groups, which are among the most common in engineering applications.

A Lie group $G$ is termed \emph{exponential} if the exponential map $\exp:\mathfrak{g}\to G$ is surjective \citep{djokovic1995exponential}, i.e., for each $\mathbf{Z}\in G$ the equation $\exp(\mathbf{Y}) = \mathbf{Z}$ has at least one solution $\mathbf{Y}\in\mathfrak{g}$. Examples of such groups include the special orthogonal group $\text{SO}(n)$ \citep[p. 28]{Gallier2020}, the special Euclidean  group $\text{SE}(n)$ \citep[p. 42]{Gallier2020}, and the Heisenberg group $\text{H}$ \citep[p. 75]{Hall2015}. Additional non-trivial examples can be found in \citet{djokovic1995exponential}. In contrast, examples of non-exponential Lie groups include the special linear group $\text{SL}(n)$ \citep[p. 28]{Gallier2020}.

For exponential groups, while the exponential map is surjective, it is not necessarily bijective. However, we can define an inverse function in the following manner. Let $\mathbb{R}_+^{n\times n}$ be the set of real matrices with positive eigenvalues. Additionally, let $\mathcal{L}^n$ represent the set of $n\times n$ real matrices whose eigenvalues $\lambda$ lie within the strip $\{\lambda : -\pi < \text{Im}(\lambda) < \pi\}$. The function $\exp:\mathcal{L}^n\to\mathbb{R}_+^{n\times n}$ is bijective, ensuring the existence of an inverse function $\Log: \mathbb{R}_+^{n\times n} \to \mathcal{L}^n$, referred to as the \emph{principal logarithm} \citep[p. 319]{Gallier2020}. Furthermore, let $\mathcal{L}_{\mathfrak{g}}^n\subseteq\mathfrak{g}$ be the subset of $n \times n$ real matrices -- elements of the Lie algebra-- whose eigenvalues $\lambda$ lie in the strip $\{\lambda : -\pi \le \text{Im}(\lambda) \le \pi\}$. We define the logarithm function $\log:G\to\mathcal{L}_{\mathfrak{g}}^n$ such that $\log(\mathbf{Z})$ coincides with $\Log(\mathbf{Z})$ when $\mathbf{Z}\in\mathbb{R}_+^{n\times n}$, and otherwise corresponds to any matrix $\mathbf{Y}\in\mathcal{L}_{\mathfrak{g}}^n$ that satisfies $\exp(\mathbf{Y}) = \mathbf{Z}$. This is feasible because we assume the group is exponential. Moreover, this choice should be predefined and deterministic.

With this, we can state the following important lemma.
\begin{lemma}\label{lemma:log-exp-log-equals-log}
    For all $\mathbf{Z}\in G$ and for all $r \in[0, 1]$, the following property holds:
    $\log\Bigl(\exp\bigl(r\log(\mathbf{Z})\bigr)\Bigr) = r\log(\mathbf{Z})$.
\end{lemma}
\begin{proof}
    The proof will be divided in three cases.
    
    \textbf{Case 1:} When $\mathbf{Z} \in \mathbb{R}_+^{n\times n}$ and $r\in[0,1]$, $\log(\mathbf{Z})=\Log(\mathbf{Z})$ and therefore $\log(\mathbf{Z})$ will lie on $\mathcal{L}^n$. Since $0\le r\le1$, then\footnote{Note that if $\lambda$ is an eigenvalue of $\mathbf{X}$, then $r\lambda$ is an eigenvalue of $r\mathbf{X}$ for any scalar $r$.} it holds that $r \log(\mathbf{Z})$ will also lie on $\mathcal{L}^n$. Moreover, since in this case, the logarithm is equal to the principal logarithm, which is invertible within its domain, it holds by definition that $\log\Bigl(\exp\bigl(r\log(\mathbf{Z})\bigr)\Bigr) = r\log(\mathbf{Z})$.

    \textbf{Case 2:} Now, let $\mathbf{Z}\notin \mathbb{R}_+^{n\times n}$ and $r\in[0, 1)$, then $\log(\mathbf{Z}) \in \mathcal{L}^n_\mathfrak{g}$. Thus $r\log(\mathbf{Z})\in\mathcal{L}^n$, where the logarithm is bijective. Therefore, the expression also holds.

    \textbf{Case 3:} Now, let $\mathbf{Z}\notin \mathbb{R}_+^{n\times n}$ and $r=1$. In this case, the expression reduces to $\log(\exp(\log(\mathbf{Z}))) = \log(\mathbf{Z})$. By the definition of $\log$, it follows that $\exp(\log(\mathbf{Z})) = \mathbf{Z}$, which ensures that the equality holds since the $\log$ function is predefined and deterministic.
\end{proof}
For the exponential Lie groups, a path $\Phi_\sigma\triangleq\Phi(\sigma, \mathbf{V}, \mathbf{W})$ can be defined as:
\begin{align}
    \Phi(\sigma, \mathbf{V}, \mathbf{W}) = \mathbf{V}\exp{\left(\log{\left(\mathbf{V}^{-1}\mathbf{W}\right)}\sigma\right)}, \label{eq:PHI-path-parameterizer-utilized-exp-of-log}
\end{align}
which is in accordance with \cref{def:PHI-path-parameterizer}. Then, an EE-distance function is defined as:
\begin{align}
    \widehat{D}(\mathbf{V}, \mathbf{W}) = \|\log{(\mathbf{V}^{-1}\mathbf{W})}\|_F.\label{eq:distance-D-hat-utilized-log-norm}
\end{align}

\begin{remark}
    The path \eqref{eq:PHI-path-parameterizer-utilized-exp-of-log} and EE-distance \eqref{eq:distance-D-hat-utilized-log-norm} reduce to the ones in \cref{ex:chainability} and \cref{ex:adriano-distance-function}, respectively, when applied to the particular case $G=T(m)$.

    Let $\mathcal{T}(\mathbf{V}) = \mathbf{v}\in\mathbb{R}^m$, $\mathcal{T}(\mathbf{W}) = \mathbf{w}\in\mathbb{R}^m$. Using the series expansion of $\log$ and $\exp$, we find that
    \begin{align}
    \begin{split}
        \mathbf{V}\exp{\bigl(\log{(\mathbf{V}^{-1}\mathbf{W})}\sigma\bigr)} 
        = \begin{bmatrix}
            \mathbf{I} & (1 - \sigma)\mathbf{v} + \sigma \mathbf{w}\\ \mathbf{0} & 1
        \end{bmatrix}.
        \end{split}
    \end{align}
    Note that $\mathcal{T}\bigl((1 - \sigma)\mathbf{V} + \sigma\mathbf{W}\bigr) = (1 - \sigma)\mathcal{T}(\mathbf{V}) + \sigma \mathcal{T}(\mathbf{W})$, the path in \cref{ex:chainability}.

    Using the series expansion of $\log$ again, $\|\log{(\mathbf{V}^{-1}\mathbf{W})}\|_F$
    $= \|\mathbf{V}^{-1}\mathbf{W} - I\|_F$. Note that $\mathbf{V}^{-1}\mathbf{W} - I$ is a matrix whose only non-zero column is the last one, equal to $[\,(\mathbf{w} - \mathbf{v})^\top\quad 0\,]^\top$, this implies that $\|\mathbf{V}^{-1}\mathbf{W} - I\|_F$$=\|\mathbf{w}-\mathbf{v}\|$, which is clearly equal to the EE-distance in \cref{ex:adriano-distance-function}. 
\end{remark}

In order to invoke \cref{thm:convergence-vector-field}, function $\widehat{D}$ in \eqref{eq:distance-D-hat-utilized-log-norm} needs to be an EE-distance (see \cref{def:distance-D-hat-arbitrary-elements}) that is left-invariant (see \cref{def:distance-left-invariant}), chainable (see \cref{def:chainable-distance}) and locally linear (see \cref{def:locallylinear}). Thus, we prove all of these properties in the following proposition.

\begin{proposition}
    Adopting the path $\Phi$ in \eqref{eq:PHI-path-parameterizer-utilized-exp-of-log}, the function $\widehat{D}$ in \eqref{eq:distance-D-hat-utilized-log-norm} is a left-invariant, chainable, and locally linear EE-distance.
\end{proposition}
\begin{proof}
    We prove each property separately. 
    
    \textbf{EE-distance}: Positive definiteness and differentiability are immediate upon inspection, and thus $\widehat{D}$ is an EE-distance.
    
    \textbf{Left-invariant}: The distance function is left-invariant since, for all $\mathbf{A} \in G$, $\widehat{D}(\mathbf{A}\mathbf{V}, \mathbf{A}\mathbf{W}) =  \|\log{(\mathbf{V}^{-1}\mathbf{A}^{-1}\mathbf{A}\mathbf{W})}\|_F$, which is clearly equal to $\widehat{D}(\mathbf{V}, \mathbf{W})$.

    \textbf{Chainable}: To prove the chainability property, we first substitute $\Phi$ by its expression \eqref{eq:PHI-path-parameterizer-utilized-exp-of-log} in \eqref{eq:distance-D-hat-utilized-log-norm}, which results in
    \begin{align}
        % \begin{split}
             \widehat{D}(\mathbf{V}, \Phi_\sigma) &= \Bigl\|\log\Bigl(\mathbf{V}^{-1}\mathbf{V}\exp\bigl(\log(\mathbf{V}^{-1}\mathbf{W})\sigma\bigr)\Bigr)\Bigr\|_F
             =\sigma\Bigl\|\log{\left(\mathbf{V}^{-1}\mathbf{W}\right)}\Bigr\|_F,
        % \end{split}
    \end{align}
    using \cref{lemma:log-exp-log-equals-log} with $\mathbf{Z}=\mathbf{V}^{-1}\mathbf{W}$ and $r=\sigma$, and the fact that $\sigma\ge0$. Now, using the fact that, by definition, $\mathbf{V}^{-1}\mathbf{W}=\exp(\log(\mathbf{V}^{-1}\mathbf{W}))$, we can express the following:
    \begin{align}
             \widehat{D}(\Phi_\sigma, \mathbf{W}) = \Bigl\|\log\Bigl(\exp\bigl(-\log(\mathbf{V}^{-1}\mathbf{W})\sigma\bigr)\exp\bigl(\log(\mathbf{V}^{-1}\mathbf{W})\bigr)\Bigr)\Bigr\|_F
    \end{align}
    Note that $\log(\mathbf{V}^{-1}\mathbf{W})$ commutes with $-\sigma\log(\mathbf{V}^{-1}\mathbf{W})$, and thus we can express the product of exponentials as the exponential of the sum of the arguments:
    \begin{align}
        \widehat{D}(\Phi_\sigma, \mathbf{W}) = \Bigl\|\log\Bigl(\exp\bigl((1-\sigma)\log(\mathbf{V}^{-1}\mathbf{W})\bigr)\Bigr)\Bigr\|_F.
    \end{align}
    Invoking \cref{lemma:log-exp-log-equals-log} with $\mathbf{Z}=\mathbf{V}^{-1}\mathbf{W}$ and $r=1-\sigma$, and using the fact that $0\le\sigma\le1$, the previous expression reduces to 
    \begin{align}
       \widehat{D}(\Phi_\sigma, \mathbf{W}) =(1-\sigma)\|\log{\left(\mathbf{V}^{-1}\mathbf{W}\right)}\|_F.
    \end{align}
    Clearly, $\widehat{D}(\mathbf{V}, \Phi_\sigma) + \widehat{D}(\Phi_\sigma, \mathbf{W}) = \|\log(\mathbf{V}^{-1}\mathbf{W})\|_F = \widehat{D}(\mathbf{V}, \mathbf{W})$. 
    
    \textbf{Locally linear}: to prove that $\widehat{D}$ is locally linear, first note that, using \cref{lemma:log-exp-log-equals-log} and the fact that $\sigma$ is non-negative, $\widehat{D}(\mathbf{V}, \Phi_\sigma) = \sigma\|\log{\left(\mathbf{V}^{-1}\mathbf{W}\right)}\|_F$, thus we have
    \begin{align}
        % \begin{split}
            \lim_{\sigma\to0^+}\frac{1}{\sigma}\widehat{D}(\mathbf{V}, \Phi_\sigma) &= \lim_{\sigma\to0^+}\frac{\sigma}{\sigma}\|\log{\left(\mathbf{V}^{-1}\mathbf{W}\right)}\|_F
            = \|\log{\left(\mathbf{V}^{-1}\mathbf{W}\right)}\|_F > 0
        % \end{split}
    \end{align}
    as long as $\mathbf{V} \not= \mathbf{W}$.
\end{proof}

\section{Explicit Construction of the EE-Distance Function in SE(3)}
As mentioned, the group \text{SE}(3) is exponential, allowing us to utilize the construction from \cref{subs:explconst}. However, instead of computing $\widehat{D}(\mathbf{V},\mathbf{W}) = \|\log(\mathbf{V}^{-1}\mathbf{W})\|_F$ through a generic algorithm to compute the matrix logarithm followed by applying the matrix norm, the structure of the group $\text{SE}(3)$ allows a more efficient and simpler approach. The algorithm for computing $\widehat{D}(\mathbf{V},\mathbf{W})$ is as follows:

\begin{itemize}
    \item Extract the rotation part $\mathbf{Q} \in \text{SO}(3)$ and the translation part $\mathbf{t} \in \mathbb{R}^3$ out of $\mathbf{V}^{-1}\mathbf{W}$.

    \item Compute $u \triangleq \frac{1}{2}\bigl(\text{Tr}(\mathbf{Q})-1\bigr)$ and $v \triangleq \frac{1}{2\sqrt{2}}\|\mathbf{Q}{-}\mathbf{Q}^{\top}\|_F$. One can see that $u = \cos(\theta)$ and $v=\sin(\theta)$, in which $\theta \in [0, \pi]$ is the rotation angle related to $\mathbf{Q}$. Compute $\theta = \text{atan2}(v,u)$.

    \item Compute $\alpha \triangleq \frac{2-2u-\theta^2}{4(1-u)^2}$. Compute
    $\mathbf{M} \triangleq  \mathbf{I}(1-2\alpha) +  (\mathbf{Q}+\mathbf{Q}^{\top})\alpha$.

    \item Finally, $\widehat{D}= \sqrt{2\theta^2 + \mathbf{t}^\top \mathbf{M}\mathbf{t}}$.

\end{itemize}

It can be shown that the result of $\|\log(\mathbf{V}^{-1}\mathbf{W})\|_F$ is independent of the choice of $\log(\mathbf{V}^{-1}\mathbf{W})$ in the edge cases where $\mathbf{V}^{-1}\mathbf{W}$ has negative eigenvalues (see the discussion in \cref{subs:explconst}). This is evident in the fact that the algorithm does not include any components that require a choice to be made.

Note that $\alpha$ is well-defined for all $\theta \in (0,\pi]$. When $\theta=0$, we just need to take the limit to obtain $\alpha=-1/12$. To identify the points of non-differentiability of $\widehat{D}$, it suffices to analyze the derivatives with the respect to the variables $\mathbf{Q}$ and $\mathbf{t}$. The analysis reveals that the only sources of non-differentiability occur when (type i) $\mathbf{Q}=\mathbf{Q}^\top$, $\mathbf{Q} \not= \mathbf{I}$ (i.e., at rotations of $\pi$ radians) or when (type ii) $\widehat{D}=0$. However, in both cases, the directional derivatives exist. Furthermore, $D_{\text{min},\mathcal{C}}$, as defined in \cref{def:distance-D-hat-arbitrary-elements}, can be taken as $\widehat{D}$ when $\theta = \pi$  and $\mathbf{t} = \mathbf{0}$, which gives $D_{\text{min},\mathcal{C}} = \sqrt{2}\pi$. Thus, when $\widehat{D} < D_{\text{min},\mathcal{C}}$, it necessarily follows that $\theta < \pi$, avoiding the non-differentiable points of type i. Additionally, when $\widehat{D} > 0$, the non-differentiable points of type ii are also avoided. Therefore, the condition $0 < \widehat{D} < \sqrt{2}\pi$ guarantees that $\widehat{D}$ is differentiable, as required in \cref{def:distance-D-hat-arbitrary-elements}.

To compute the terms of the vector field, we need to compute the derivative $\text{L}_{\mathbf{V}}[\widehat{D}](\mathbf{H},\mathbf{H}_d(s^*))$. While this can be done analytically, we believe it is simpler to implement a numerical approach by evaluating the left-hand side of \eqref{eq:Leq} for $\boldsymbol{\zeta} = \mathbf{e}_i$ with a small $\epsilon$.

% \begin{comment}
\section{Analytic Derivative}
To compute $\boldsymbol{\xi}_N$ according to \cref{def:normal-vector}, we need to compute $\frac{\partial \widehat{D}}{\partial \mathbf{v}_i}$, $i \in [1,4]$. By the chain rule:

\begin{equation}
    \frac{\partial \widehat{D}}{\partial \mathbf{v}_i} (\mathbf{V},\mathbf{W}) =  \frac{\partial \widehat{E}}{\partial \mathbf{Z}_i}(\mathbf{W}^{-1}\mathbf{V}) \mathbf{W}^{-1}.
\end{equation}

Thus, it suffices to show how to compute $\frac{\partial \widehat{E}}{\partial \mathbf{Z}_i}$. This can be taken as the $i^{th}$ row of the $4 \times 4$ matrix:
\begin{equation}
    \frac{\partial \widehat{E}}{\partial \mathbf{Z}}(\mathbf{Z}) \triangleq \left[\begin{array}{cc} \frac{\partial \widehat{E}}{\partial \mathbf{Q}}( \mathbf{Z}) & \mathbf{0} \\
    \frac{\partial \widehat{E}}{\partial \mathbf{t}}( \mathbf{Z}) & 0 \end{array}\right]
\end{equation}
\noindent in which, as in Algorithm 1, $\mathbf{Q}$ and $\mathbf{t}$ are the rotation matrix and 3D translation vectors of $\mathbf{Z}$, respectively,  $ \frac{\partial \widehat{E}}{\partial \mathbf{Q}}$ the $3 \times 3$ matrix in which the entry in row $i$ and column $j$ is $\frac{\partial \widehat{E}}{\partial Q_{ji}}$ and $\frac{\partial \widehat{E}}{\partial \mathbf{t}}$ the $1 \times 3$ row vector in which the column $j$ is $\frac{\partial \widehat{E}}{\partial t_j}$. The algorithms for computing the two elements of this matrix is as follows (in which $u, v, \theta$, $\alpha$ and $\mathbf{M}$ were defined in Algorithm 1):

\begin{itemize}
    \item Compute 
    \begin{eqnarray}
       &&  \alpha'(\theta) = \frac{ \theta {+} (1 {-}\theta^2) v{-} (\theta+v)u}{2(u-1)^3} \ , \ \mathbf{N} {\triangleq} \frac{\mathbf{Q}{+}\mathbf{Q}^\top}{2}{-}\mathbf{I} \nonumber \\
       && \frac{\partial \theta}{\partial \mathbf{Q}} = \frac{u}{4v}(\mathbf{Q}^{\top}-\mathbf{Q}) - \frac{v}{2}\mathbf{I}.
    \end{eqnarray}
    \item Finally, compute 
    \begin{eqnarray}
        && \frac{\partial \widehat{E}}{\partial \mathbf{Q}} = \frac{1}{\widehat{E}}\Bigg(\big(2\theta + \alpha' \mathbf{t}^{\top}\mathbf{N}\mathbf{t} \big) \frac{\partial \theta}{\partial \mathbf{Q}} + \alpha \mathbf{t}\mathbf{t}^{\top}\Bigg). \nonumber \\
        && \frac{\partial \widehat{E}}{\partial \mathbf{t}} = \frac{1}{\widehat{E}} \mathbf{t}^{\top} \mathbf{M} .
    \end{eqnarray}
\end{itemize}
% \end{comment}