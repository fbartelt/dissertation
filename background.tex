% !TeX root = main.tex
\chapter{Thoeretical Background}\label{ch:background}
In this chapter we revisit the vector field strategy in the euclidean space based on a curve parametrization \citep{Rezende2022}, as it will prove valuable in drawing connections between both works. We also present some fundamental concepts of Lie groups and Lie algebras, which will be essential for the development of our extension.

\section{Vector Field in Euclidean Space}\label{sec:adriano-review}
For clarity, we revisit the vector field strategy presented in \citet{Rezende2022}. Since our work extends this previous approach, this review will help establish direct connections between both works and highlight the core aspects of our contributions. The primary goal of the authors in \citet{Rezende2022} is to develop an artificial $n$-dimensional vector field that guides system trajectories toward a predefined curve and ensures circulation around it. A key element of this formulation is the definition of a distance function with essential properties. As their work focuses solely on Euclidean space, they adopt the Euclidean distance for the vector field computation, which is derived using a parametric representation of the curve.

We summarize the main steps for constructing this vector field, emphasizing the most critical properties. Although the original paper addresses time-varying curves, we limit our discussion to the static portion of the methodology. The authors consider a system modeled as the following single integrator 
\begin{align}
    \dot{\mathbf{h}} = \boldsymbol{\xi}, \label{eq:adriano-single-integrator}
\end{align}
where $\mathbf{h}\in\mathbb{R}^m$ represents the system state, and $\boldsymbol{\xi}\in\mathbb{R}^m$ denotes the system input. The objective is to compute a vector field $\Psi:\mathbb{R}^m\to\mathbb{R}^m$ such that, if the system input is equal to the vector field, the system trajectories converge to and follow the target curve $\mathcal{C}$, for which a parametrization is given by $\mathbf{h}_d(s)$. Despite relying on a parametric representation of the curve, it is important to note that the resulting computations are independent of the specific parametrization chosen.

The authors define their distance function $D$ as the Euclidean distance between the system's current state and the nearest point on the curve, i.e., 
\begin{align}
    D(\mathbf{h}) \triangleq \min_{s}\widehat{D}(\mathbf{h}, \mathbf{h}_d(s))=\min_{s}\|\mathbf{h}- \mathbf{h}_d(s)\|. \label{eq:adriano-EC-distance}   
\end{align}
In this context, we denote $s^*$ as the optimal parameter such that $\mathbf{h}_d(s^*(\mathbf{h}))$ is the closest point on the curve to the current state.
\begin{figure}
    \centering
    \def\svgwidth{.8\linewidth}
    \import{figures/}{plotly_vf2.pdf_tex}
    \caption{Example showing the vector field and the components for a point $\mathbf{h}\in G$ and curve $\mathcal{C}$.}
    \label{fig:vector-field-adriano}
\end{figure}

Next, the authors introduce two components of their vector field: the \emph{normal} component $\boldsymbol{\xi}_{N}$, responsible for convergence, and the \emph{tangent} component $\boldsymbol{\xi}_{T}$, which ensures circulation. The resulting expression for the vector field is 
\begin{align}
    \Psi(\mathbf{h}) = k_N(\mathbf{h})\boldsymbol{\xi}_{N}(\mathbf{h}) + k_T(\mathbf{h})\boldsymbol{\xi}_{T}(\mathbf{h}), \label{eq:adriano-vector-field-expression}    
\end{align}
where $k_N$ and $k_T$ are gains, dependent on the system state, that balance the predominance of the normal and tangent components. This vector field strategy is illustrated in \autoref{fig:vector-field-adriano}. The normal component, $\boldsymbol{\xi}_{N}$, is naturally taken as the negative gradient of the distance function, due to the use of Euclidean distance:
\begin{align}
    \boldsymbol{\xi}_{N} = -\nabla D.
\end{align}

There is a key aspect of the normal component that is crucial for the convergence proof using Lyapunov stability theory: the fact that the time derivative of the distance function can be expressed as $\dot{D}=-\boldsymbol{\xi}_{N}^{\top}{\boldsymbol{\xi}}$. We emphasize the significance of this feature, as it will play an important role in our extension. In our approach, the normal component is similarly constructed by identifying the term that arises when differentiating the distance function.

Next, we address the tangent component. This component is solely related to the target curve and is defined as the tangent vector at the nearest point on the curve, i.e.,
\begin{align}
    \boldsymbol{\xi}_{T}(\mathbf{h}) = \frac{d}{ds}\mathbf{h}_d(s)|_{s=s^*(\mathbf{h})}.
\end{align}
A noteworthy property of both components is that they are orthogonal to each other, which is essential in the proof of convergence for this algorithm.

In the Lyapunov stability proof, the final result shows that the time derivative of $D$ is negative semidefinite. The proof is then completed using two other essential properties: the fact that the distance function has no local minima outside the curve, and the fact that the ``generalized gradient'' of this function never vanishes. With these features, the authors demonstrate that if the system trajectories follow the vector field, the system will converge to and circulate around a predefined curve. A summary of these key features is as follows:
\begin{feature}
    \item The time derivative of the distance function is the opposite of the dot product between the so called \emph{normal} component and the system control input; \label{feat:adriano-time-derivative-lyapunov-normal-comp}
    \item The \emph{normal} and \emph{tangent} components are orthogonal to each other; \label{feat:adriano-orthogonality}
    \item The distance function has no local minima outside the target curve. Furthermore, whenever the distance function is differentiable, its gradient never vanishes. \label{feat:adriano-no-local-minima}
\end{feature}
In our generalization, we will incorporate and build upon these features.

\section{Lie groups and Lie algebras}\label{sec:background-lie-theory}
In this section, we introduce \emph{Lie groups}, which are smooth manifolds that are also groups, and \emph{Lie algebras}. Besides providing examples of both, we also introduce concepts that will prove valuable in our work. This section integrates concepts and ideas from the following books: \citet{Lee2012,Gallier2020,Hall2015,Duistermaat2012}, with the aim of presenting the concepts in a clearer and more accessible manner. We begin with the general definition of a Lie group.
\subsection{Lie groups}
\begin{definition}[Lie group]
    A \emph{Lie group} is a smooth manifold $G$ satisfying the following properties:
    \begin{property}
        \item $G$ is a group (with identity element denoted $\iota$)
        \item $G$ is a topological group (the group operation and the inverse map are continuous). Furthermore, the group operation $\circ: G\times G\to G$ and the inverse map $\cdot^{-1}: G\to G$ are smooth.
    \end{property}
\end{definition}

The mapping between Lie groups is a fundamental concept and will play an important role in our extension, specifically for drawing connections between our extension and the vector field strategy in Euclidean space. Usually these mappings have additional properties and are defined as follows:
\begin{definition}
    If $G$ and $H$ are Lie groups, a \emph{homomorphism} $\mathcal{H}: G\to H$ is a smooth map (between manifolds $G$ and $H$) that is also a group homomorphism, that is, if $\circ$ and $\star$ are the group operations in $G$ and $H$, respectively, then $\mathcal{H}(\mathbf{X}\circ\mathbf{Y}) = \mathcal{H}(\mathbf{X})\star\mathcal{H}(\mathbf{Y})$ for all $\mathbf{X},\mathbf{Y}\in G$. If this map is also a diffeomorphism (the inverse map $\mathcal{H}^{-1}$ is also a homomorphism), then $\mathcal{H}$ is an \emph{isomorphism}. In this case, we say that $G$ and $H$ are \emph{isomorphic} and denote this by $G\cong H$.
\end{definition}

There are two important isomorphisms in Lie groups that are important for the concept of Lie algebras. These are the translation isomorphisms, which are defined as follows:
\begin{definition}
    Given a Lie group $G$, we define two operations: the left translation $\mathcal{L}_{\mathbf{X}}: G\to G$ and the right translation $\mathcal{R}_{\mathbf{X}}: G\to G$ for any $\mathbf{X}\in G$. These operations are defined as 
    \begin{align}
        \mathcal{L}_{\mathbf{X}}(\mathbf{Y}) &= \mathbf{X}\circ\mathbf{Y}\\
        \mathcal{R}_{\mathbf{X}}(\mathbf{Y}) &= \mathbf{Y}\circ\mathbf{X},
    \end{align}
    respectively. Since the group operation and the inverse map are smooth, the left and right translations are diffeomorphisms, and thus isomorphisms.
\end{definition}

\subsubsection{Matrix Lie Groups}
In general lines, Lie groups that can be represented by matrices are called \emph{matrix Lie groups}. These groups are defined by a set of matrices that satisfy the group properties. The group operation is matrix multiplication, the identity element is the identity matrix, and the inverse map is the matrix inverse. For a more precise definition, we need to define the \emph{general linear group} and the concept of \emph{convergence} of a sequence of matrices.
\begin{definition}
    The \emph{general linear group} over the real numbers, denoted $\text{GL}(n, \mathbb{R})$, is the set of all invertible $n\times n$ matrices with real entries. The general linaer group over the complex numbers, denoted $\text{GL}(n, \mathbb{C})$, is the group of all invertible $n\times n$ matrices with complex entries.
\end{definition}
\begin{definition}
    Let $\mathbf{X}_k$ be a sequence of complex matrices in $\mathbb{C}^{n\times n}$. We say that $\mathbf{X}_k$ \emph{converges} to a matrix $\mathbf{Y}$ if each entry of $\mathbf{X}_k$ converges to the corresponding entry of $\mathbf{Y}$ as $k\to\infty$. This means that for each $i,j\in[1,n]$, the sequence $\{\mathbf{X}_k\}_{ij}$ converges to $\mathbf{Y}_{ij}$.
\end{definition}
With both definitions, we can now define a matrix Lie group formally.
\begin{definition}
    A \emph{matrix Lie group} is a subgroup $G$ of $\text{GL}(n, \mathbb{C})$ with the property that if $\mathbf{X}_k$ is a sequence of matrices in $G$, and $\mathbf{X}_k$ converges to some matrix $\mathbf{Y}$, then either $\mathbf{Y}$ is in $G$ or $\mathbf{Y}$ is not invertible. This property is equivalent to sayint that $G$ is a \emph{closed subgroup} of $\text{GL}(n, \mathbb{C})$.
\end{definition}

Although present in the context of any Lie group, we can define two interesting properties more easily for matrix Lie groups. The knowledge of \emph{compactness} and \emph{connectedness} can reduce complexity in the study of these groups.
\begin{definition}
    A matrix Lie group $G\subset\text{GL}(n,\mathbb{C})$ is said to be \emph{compact} if and only if:
    \begin{property}
        \item whenever a sequence $\mathbf{X}_k$ is in $G$ and $\mathbf{X}_k$ converges to $\mathbf{Y}$, then $\mathbf{Y}$ is in $G$;
        \item there exists a constant $C>0$ such that for all $\mathbf{X}\in G$, $\|\{\mathbf{X}\}_{ij}\|\leq C$ for all $i,j\in[1,n]$.
    \end{property}
\end{definition}
\begin{definition}
    A matrix Lie group $G$ is termed \emph{connected} if for all $\mathbf{X},\mathbf{Y}\in G$, there exists a continuous path $\mathbf{G}(\sigma)\in G$, $a\le\sigma\le b$ such that $\mathbf{G}(a) = \mathbf{X}$ and $\mathbf{G}(b) = \mathbf{Y}$. The \emph{identity component} of $G$, denoted $G_\iota$, is the set $\mathbf{X}\in G$ for which there exists a continuous path $\mathbf{G}(\sigma)\in G$, $a\le\sigma\le b$ such that $\mathbf{G}(a) = \iota=\mathbf{I}$ and $\mathbf{G}(b) = \mathbf{X}$.
\end{definition}

\subsubsection{Examples of Lie groups}
In this section we present some examples of the most common Lie groups, which will also introduce the notation and concepts that will be used in our extension.
\begin{example}\label{ex:general-linear-group-special-linear-group}
    The \emph{special linear group} $\text{SL}(n, \mathbb{R})$ (resp. $\text{SL}(n, \mathbb{C})$) is a subgroup of $\text{GL}(n, \mathbb{R})$ (resp. $\text{GL}(n, \mathbb{C})$) consisting of invertible matrices with determinant equal to 1.
\end{example}
\begin{example}
    The group formed by the direct product of Lie groups $G = G_1 \times G_2 \times \dots \times G_k$ is a Lie group \citep[p. 152]{Lee2012}. It is also true that the semidirect product of Lie groups $G = G_1\rtimes G_2$ is also a Lie group \citep[p. 168]{Lee2012}.
\end{example}
\begin{example}\label{ex:orthogonal-group-special-orthogonal-group}
    The \emph{orthogonal group} $\text{O}(n)$ is the Lie group of distance-preserving transformation and comprises rotations and reflections. It consists of all $n\times n$ orthogonal matrices:
    \begin{align}
        \text{O}(n) = \left\{\mathbf{X}\in\mathbb{R}^{n\times n} | \mathbf{X}^T\mathbf{X} = \mathbf{I}_n\right\}.
    \end{align}

    Its subgroup $\text{SO}(n)$, the \emph{special orthogonal group}, consists of all orthogonal matrices with determinant equal to 1. This group represents rotations and as such are formed by the set of rotation matrices:
    \begin{align}
        \text{SO}(n) = \left\{\mathbf{X}\in\mathbb{R}^{n\times n} | \mathbf{X}^T\mathbf{X} = \mathbf{I}_n,\, \det(\mathbf{X}) = 1\right\}.
    \end{align}
\end{example}
\begin{example}\label{ex:euclidean-group-special-euclidean-group}
    The \emph{Euclidean group} $\text{E}(n)$ is a Lie group of isometries in Euclidean space. This group is formed by the semidirect product $\mathbb{R}^n \rtimes \text{O}(n)$ and is defined as
    \begin{align}
        \text{E}(n) = \left\{\begin{bmatrix}
            \mathbf{R} & \mathbf{p} \\ \mathbf{0} & 1
        \end{bmatrix} \in \mathbb{R}^{(n+1)\times(n+1)} | \mathbf{R}\in\text{O}(n),\, \mathbf{p}\in\mathbb{R}^n\right\}.
    \end{align}

    Its subgroup $\text{SE}(n)$, the \emph{special Euclidean group}, consists of all matrices in the Euclidean group with determinant equal to 1, formed by the semidirect product $\mathbb{R}^n\rtimes \text{SO}(n)$. This group represents rigid transformations and is formed by the set of homogeneous transformation matrices:
    \begin{align}
        \text{SE}(n) = \left\{\begin{bmatrix}
            \mathbf{R} & \mathbf{p} \\ \mathbf{0} & 1
        \end{bmatrix} \in \mathbb{R}^{(n+1)\times(n+1)} | \mathbf{R}\in\text{SO}(n),\, \mathbf{p}\in\mathbb{R}^n\right\}.
    \end{align}
\end{example}
\begin{example}\label{ex:translation-group}
    The group $(\mathbb{R}^n,+)$, also denoted $\mathbb{R}^n$, is also a Lie group, where the group operation is the vector addition, the identity element is the zero vector, and the inverse map is the negation of the vector. We will usually represent this group through an inclusion map $\mathbb{R}^n \hookrightarrow \text{SE}(n)$ and denote the group under this inclusion the \emph{translation group} $\text{T}(n)$, defined as
    \begin{align}
        \text{T}(n) = \left\{\begin{bmatrix}
            \mathbf{I}_n & \mathbf{p} \\ \mathbf{0} & 1
        \end{bmatrix} \in \text{SE}(n) | \mathbf{p}\in\mathbb{R}^n\right\}.
    \end{align}
\end{example}
\begin{example}\label{ex:independent-translation-rotation-ISE}
    Although not present in the literature, we introduce the group of independent translations and rotations (\emph{Independent Special Euclidean group}) $\text{ISE}(n)\cong\mathbb{R}^n\times\text{SO}(n)$, defined as
    \begin{align}
        \begin{split}
            \text{ISE}(n) &= \left\{\begin{bmatrix}
            \mathbf{R} & \mathbf{0}\\
            \mathbf{0} & \mathbf{P}\\
            \end{bmatrix}\in\mathbb{R}^{(2n+1)\times(2n+1)}\,|\, \mathbf{R}\in\text{SO}(n),\,\mathbf{P}\in\text{T}(n)\right\}\\
            &= \left\{\begin{bmatrix}
            \mathbf{R} & \mathbf{0} & \mathbf{0}\\
            \mathbf{0} & \mathbf{I} & \mathbf{p}\\
            \mathbf{0} & \mathbf{0} & 1
            \end{bmatrix}\in\mathbb{R}^{(2n+1)\times(2n+1)}\,|\, \mathbf{R}\in\text{SO}(n),\,\mathbf{p}\in\mathbb{R}^n\right\}.
        \end{split} \label{eq:ISE-group}
    \end{align}
    Although they are isomorphic, we will use the representation $\mathbb{R}^n\times \text{SO}(n)$ to denote tuples of translations and rotations $(\mathbf{p},\mathbf{R})$, and the representation $\text{ISE}(n)$ to denote elements represented as the matrices in \eqref{eq:ISE-group}. Note that we can use both representations interchangeably, since we can easily extract the position and orientation from the matrix representation by means of matrix multiplication.
\end{example}
\begin{example}\label{ex:symplectic-group}
    The \emph{real symplectic group} $\text{Sp}(2n)$ is the group of all $2n\times 2n$ real matrices that preserve a non-degenerate skew-symmetric bilinear form $\omega$. This group plays an important role in classical mechanics, specially in the study of Hamiltonian systems. The group is defined as
    \begin{align}
        \text{Sp}(2n) &= \left\{\mathbf{X}\in\mathbb{R}^{2n\times 2n} | \mathbf{X}^\top\boldsymbol{\Omega}\mathbf{X} = \boldsymbol{\Omega}\right\}\\
        &= \left\{\mathbf{X}\in\mathbb{R}^{2n\times 2n} | -\boldsymbol{\Omega}\mathbf{X}^\top\boldsymbol{\Omega} = \mathbf{X}^{-1}\right\},
    \end{align}
    where 
    \begin{align}
        \boldsymbol{\Omega} = \begin{bmatrix}
            \mathbf{0} & \mathbf{I}_n\\
            -\mathbf{I}_n & \mathbf{0}
        \end{bmatrix}.
    \end{align}
    The skew-symmetric bilinear form can be characterized by $\omega(x,y)=\langle x, \boldsymbol{\Omega}y\rangle\,\forall\,x,y\in\mathbb{R}^{2n}$. We can also define the \emph{complex symplectic group} $\text{Sp}(2n, \mathbb{C})$ as the group of all $n\times n$ complex matrices that preserve the same non-degenerate skew-symmetric bilinear form, thus the group definition is the same, but with matrices with complex entries.
\end{example}
\begin{example}
    The \emph{indefinite orthogonal group} (or generalized orthogonal group) $\text{O}(p,q)$ is the group of all $n\times n$ real matrices
    \begin{align}
        \text{O}(p,q) = \left\{\mathbf{X}\in\mathbb{R}^{n\times n} | \mathbf{X}^\top\mathbf{I}_{p,q}\mathbf{X} = \mathbf{I}_{p,q}\right\},
    \end{align}
    where
    \begin{align}
        \mathbf{I}_{p,q} = \begin{bmatrix}
            \mathbf{I}_p & \mathbf{0}\\
            \mathbf{0} & -\mathbf{I}_q
        \end{bmatrix},
    \end{align}
    and $n=p+q$.

    Similarly, we can define the \emph{indefinite special orthogonal group} $\text{SO}(p,q)$ as the group
    \begin{align}
        \text{SO}(p, q) = \left\{\mathbf{X}\in\text{O}(p, q) | \det\mathbf{X}=1\right\}.
    \end{align}
    The group $\text{O}(1,3)$ is of particular interest in the study of special relativity, and is denoted the \emph{Lorentz group}. This group preserves the Lorentz metric
    \begin{align}
        (t,x,y,z) \mapsto t^2 - x^2 - y^2 - z^2.
    \end{align}
    The Lorentz group is also represented as $\text{O}(3,1)$, which would preserve the metric $x^2 + y^2 + z^2 - t^2$.
    If we restrict this group to transformations that preserve orientation, we obtain the \emph{proper Lorentz group} $\text{SO}(1,3)$. Furthermore, the subgrtoup of all Lorentz transformations that preserve orientation and the time direction is called the  \emph{proper orthochronous Lorentz group} $\text{SO}^+(1,3)$, this is the identity component $G_\iota$ of $\text{SO}(1,3)$.
    
\end{example}
% ADD Examples for indefinite orthogonal (with lorentz), heisenberg?, and symplectic groups
\begin{example}\label{ex:non-matrix-lie-group}
    An example of a Lie group that is not a matrix Lie group is given in \citet[p. 25]{Hall2015} and reproduced here:

    The group $G = \mathbb{R} \times \mathbb{R} \times \mathbb{S}^1 = \left\{(x, y, \theta) | x \in \mathbb{R}, y \in \mathbb{R}, \theta \in \mathbb{S}^1\subset\mathbb{C}\right\}$, equipped with the group operation
    \begin{align}
        (x_1, y_1, \theta_1)\circ (x_2, y_2, \theta_2) = (x_1 + x_2, y_1 + y_2, e^{ix_1y_2}\theta_1\theta_2),
    \end{align}
    is a Lie group without a matrix representation. The identity element is $\iota=(0, 0, 1)$, and the inverse map is $(x, y, \theta)^{-1} = (-x, -y, e^{ixy}\theta^{-1})$.
\end{example} 

\subsection{Lie algebras}
Lie groups are closely associated with a mathematical structure called the \emph{Lie algebra}, which allows us to study their properties using linear algebra. Usually the Lie algebra of a Lie group $G$ is denoted as the set of all smooth \emph{left-invariant} vector fields on $G$, denoted $\text{Lie}(G)$. These are vector fields that remain unchanged when ``shifted'' by the group operations. To define left-invariant vector fields formally, we first need to understand the concept of differentials in the context of Lie groups.

Suppose $\mathcal{H}:G\to H$ is a homomorphism between Lie groups $G$ and $H$. If $V$ is a vector field on $G$, then at every point $\mathbf{X}\in G$, the differential of $\mathcal{H}$ at $\mathbf{X}$, denoted $d(\mathcal{H})_\mathbf{X}$, maps the vector $V(\mathbf{X})$ in the tangent space of $G$ at $\mathbf{X}$ to a vector in the tangent space of $H$ at $\mathcal{H}(\mathbf{X})$, i.e. $d(\mathcal{H})_\mathbf{X}:T_\mathbf{X}G\to T_{\mathcal{H}(\mathbf{X})}H$.

Furthermore, if there exists a vector field $W$ on $H$ such that $d(\mathcal{H})_\mathbf{X}\bigl(V(\mathbf{X})\bigr) = W\bigl(\mathcal{H}(\mathbf{X})\bigr)$ for all $\mathbf{X}\in G$, then $V$ and $W$ are said to be $\mathcal{H}$-related. This concept is important since if $\mathcal{H}$ is a diffeomorphism, a Lie group isomorphism in this context, then there is a unique vector field $W$ on $H$ that is $\mathcal{H}$-related to $V$ \citep[p. 183]{Lee2012}.

Finally, the concept of left-invariant (resp. right-invariant) vector fields on Lie groups can be introduced by means of the differential mappings of these translations:
\begin{definition}
    A vector field $V$ on a Lie group $G$ is said to be \emph{left-invariant} (resp. \emph{right-invariant}) if it is invariant under all left (resp. right) translations:
    \begin{align*}
        d(\mathcal{L}_\mathbf{X})_\mathbf{Y}(V(\mathbf{Y})) &= V(\mathbf{X}\circ\mathbf{Y}) \,\forall\,\mathbf{X},\mathbf{Y}\in G,\\
        \text{(resp.) }d(\mathcal{R}_\mathbf{X})_\mathbf{Y}(V(\mathbf{Y})) &= V(\mathbf{Y}\circ\mathbf{X}) \,\forall\,\mathbf{X},\mathbf{Y}\in G.
    \end{align*}  
\end{definition}
Since the translation operations are isomorphisms, it follows that every left-invariant (resp. right-invariant) vector field on a Lie group can be identified with its value at the identity element. In other words, if $V$ is a left-invariant (resp. right-invariant) vector field and $V_\iota\triangleq V(\iota) \in T_\iota G$ is the value of the vector field at the identity element of $G$, then $V$ and $V_\iota$ are $\mathcal{L}_\mathbf{X}$-related (resp. $\mathcal{R}_\mathbf{X}$-related) for all $\mathbf{X}\in G$, which implies that $V(\mathbf{X}) = d(\mathcal{L}_\mathbf{X})_\iota V_\iota$ (resp. $V(\mathbf{X}) = d(\mathcal{R}_\mathbf{X})_\iota V_\iota$).

From the last statement, it is clear that the Lie algebra is a vector space that captures the local behavior of the group around the identity element. For this reason, the Lie algebra of a Lie group $G$ is also defined as the tangent space at the identity element $T_\iota G$, commonly denoted $\mathfrak{g}$. More generally, a Lie algebra is defined as follows:
\begin{definition}
    A \emph{real Lie algebra} is a real vector space $\mathfrak{g}$, endowed with a map $[\cdot, \cdot]:\mathfrak{g}\times\mathfrak{g}\to\mathfrak{g}$, called the \emph{Lie bracket} on $\mathfrak{g}$, that satisfies the following properties for all $\mathbf{A},\mathbf{B},\mathbf{C}\in\mathfrak{g}$:
    \begin{property}
        \item \emph{Bilinearity}: For all $a,b\in\mathbb{R}$,
        \begin{align*}
            [a\mathbf{A}+b\mathbf{B}, \mathbf{C}] &= a[\mathbf{A}, \mathbf{C}] + b[\mathbf{B}, \mathbf{C}], \\
             [\mathbf{C}, a\mathbf{A}+b\mathbf{B}] &= a[\mathbf{C}, \mathbf{A}] + b[\mathbf{C}, \mathbf{B}];
        \end{align*}
        \item \emph{skew-symmetry}: 
        \begin{align*}
            [\mathbf{A}, \mathbf{B}] = -[\mathbf{B}, \mathbf{A}];
        \end{align*} \label{prop:lie-algebra-skew-symmetry}
        \item \emph{Jacobi identity}:
        \begin{align*}
            [\mathbf{A}, [\mathbf{B}, \mathbf{C}]] + [\mathbf{B}, [\mathbf{C}, \mathbf{A}]] + [\mathbf{C}, [\mathbf{A}, \mathbf{B}]] = 0.
        \end{align*}
    \end{property}
\end{definition}
Note that \cref{prop:lie-algebra-skew-symmetry} implies that $[\mathbf{A}, \mathbf{A}]=0\,\forall\,\mathbf{A}\in\mathfrak{g}$. 

Mappings between Lie algebras also have an important role in the study of Lie groups, specially homomorphisms and isomorphisms. The definition of a homomorphism between Lie algebras is given as follows:
\begin{definition}
    If $\mathfrak{g}$ and $\mathfrak{h}$ are Lie algebras with Lie brackets $[\cdot, \cdot]_\mathfrak{g}$ and $[\cdot, \cdot]_\mathfrak{h}$, respectively, a linear map $\mathscr{H}:\mathfrak{g}\to\mathfrak{h}$ is denoted a \emph{homomorphism} if it preserves the Lie bracket, that is, $\mathscr{H}([\mathbf{A}, \mathbf{B}]_\mathfrak{g}) = [\mathscr{H}(\mathbf{A}), \mathscr{H}(\mathbf{B})]_\mathfrak{h}\,\forall\, \mathbf{A},\mathbf{B}\in\mathfrak{g}$. Furthermore, if $\mathscr{H}$ is a bijection, then $\mathscr{H}^{-1}$ is also a homomorphism, and $\mathscr{H}$ is an \emph{isomorphism}. In this case, we say that $\mathfrak{g}$ and $\mathfrak{h}$ are \emph{isomorphic} and denote this by $\mathfrak{g}\cong\mathfrak{h}$.
\end{definition}
Lie algebra homomorphisms can also be induced by Lie group homomorphisms. If $\mathcal{H}$ is a Lie group homomorphism between Lie groups $G$ and $H$, then the differential of $\mathcal{H}$ at the identity element $d\mathcal{H}_\iota=\mathscr{H}$ is a Lie algebra homomorphism between the Lie algebras $\mathfrak{g}=T_\iota G$ and $\mathfrak{h}=T_\iota H$ \citep[p. 41]{Duistermaat2012}. Furthermore, if $\mathcal{H}$ is a Lie group isomorphism, then $d\mathcal{H}_\iota$ is a Lie algebra isomorphism.

The most important Lie algebra homomorphism is the differential of the translation operations. As previously seem, every left-invariant (resp. right-invariant) vector field can be identified with the tangent space at the identity element, and since the translation operations are isomorphisms, the space of left-invariant (resp. right-invariant) vector fields $\text{Lie}(G)$ is isomorphic to the tangent space at the identity element $\mathfrak{g}$, i.e. $\text{Lie}(G)\cong\mathfrak{g}$. Both definitions of the Lie algebra of a Lie group are equally important and are used interchangeably depending on the context, as one definition might allow for easier comprehension or computtions. The concept of homomorphisms thus tie both definitions together.
\subsubsection{Examples of Lie algebras}
In this section we present the corresponding ones for the Lie groups presented in the previous section, as well as some Lie algebras isomorphisms.
\begin{example}
    The Lie algebra of the general linear group $\text{GL}(n, \mathbb{R})$ is the set of all $n\times n$ matrices, denoted $\mathfrak{gl}(n, \mathbb{R})$, with its Lie bracket given by the commutator $[\mathbf{A}, \mathbf{B}] = \mathbf{A}\mathbf{B} - \mathbf{B}\mathbf{A}$. Similarly, the Lie algebra of the general linear group $\text{GL}(n, \mathbb{C})$, denoted $\mathfrak{gl}(n, \mathbb{C})$, is the set of all $n\times n$ complex matrices with the same Lie bracket. Furthermore, the Lie algebra of the special linear group $\text{SL}(n, \mathbb{R})$ (resp. $\text{SL}(n, \mathbb{C})$) is the set of all $n\times n$ real (resp. complex) matrices with zero trace, denoted $\mathfrak{sl}(n, \mathbb{R})$ (resp. $\mathfrak{sl}(n, \mathbb{C})$).
\end{example}
\begin{example}
    The lie algebra for the direct product of Lie groups $G=G_1 \times G_2 \times \dots \times G_k$ is the direct sum of the Lie algebras of the groups, i.e. $\mathfrak{g}_1\oplus\mathfrak{g}_2\oplus\dots\oplus\mathfrak{g}_k$.
\end{example}
\begin{example}
    The Lie algebra of the orthogonal group $\text{O}(n)$ is the same as the Lie algebra of the special orthogonal group $\text{SO}(n)$, denoted $\mathfrak{so}(n)$. This Lie algebra is the set of all $n\times n$ skew-symmetric matrices:
    \begin{align*}
        \mathfrak{so}(n) = \left\{\mathbf{A}\in\mathbb{R}^{n\times n} | \mathbf{A}^\top = -\mathbf{A}\right\}.
    \end{align*}
    In the special case of the special orthogonal Lie group $\text{SO}(3)$, with Lie algebra $\mathfrak{so}(3)$, a common basis is given by the of matrices
    \begin{align*}
        \mathbf{E}_1 = \begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & -1 \\ 0 & 1 & 0 \end{bmatrix}, \quad \mathbf{E}_2 = \begin{bmatrix} 0 & 0 & 1 \\ 0 & 0 & 0 \\ -1 & 0 & 0 \end{bmatrix}, \quad \mathbf{E}_3 = \begin{bmatrix} 0 & -1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}.
    \end{align*}
    It is usual to represent elements of $\mathfrak{so}(3)$ through a map $\SL$ that maps a vector $\boldsymbol{\zeta} = [\zeta_1\ \zeta_2\ \zeta_3]^\top$ into a skew-symmetric matrix:
    \begin{align*}
        \SL[\boldsymbol{\zeta}] = \zeta_1 \mathbf{E}_1 + \zeta_2 \mathbf{E}_2 + \zeta_3 \mathbf{E}_3
        = \begin{bmatrix} 0 & -\zeta_3 & \zeta_2 \\ \zeta_3 & 0 & -\zeta_1 \\ -\zeta_2 & \zeta_1 & 0 \end{bmatrix}.
    \end{align*}
    In fact, this map is an isomorphism between the Lie algebra of $\mathbb{R}^3$ and $\mathfrak{so}(3)$. Let the Lie bracket of $\mathbb{R}^3$ be the cross product, $\mathbf{a} = [a_1\ a_2\ a_3]^\top$ and $\mathbf{b} = [b_1\ b_2\ b_3]^\top$, then 
    \begin{align*}
        \SL[\mathbf{a}\times \mathbf{b}] &= \begin{bmatrix}
            0 & a_2b_1-a_1b_2 & a_3b_1-a_1b_3\\
            a_1b_2 - a_2b_1 & 0 & a_3b_2-a_2b_3\\
            a_1b_3-a_3b_1 & a_2b_3-a_3b_2 & 0
        \end{bmatrix},
    \end{align*}
    now computing $\SL[\mathbf{a}]\SL[\mathbf{b}]$ and $\SL[\mathbf{b}]\SL[\mathbf{a}]$ gives
    \begin{align*}
        \SL[\mathbf{a}]\SL[\mathbf{b}] &=
        \begin{bmatrix}
            -a_3b_3 -a_2b_2 & a_2b_1 & a_3b_1\\
            a_1b_2 & -a_3b_3 - a_1b_1 & a_3b_2\\
            a_1b_3 & a_2b_3 & -a_2b_2 - a_1b_1
        \end{bmatrix},\\
        \SL[\mathbf{b}]\SL[\mathbf{a}] &=
        \begin{bmatrix}
            -a_3b_3 -a_2b_2 & a_1b_2 & a_1b_3\\
            a_2b_1 & -a_3b_3 - a_1b_1 & a_2b_3\\
            a_3b_1 & a_3b_2 & -a_2b_2 - a_1b_1
        \end{bmatrix}.
    \end{align*}
    Clearly, $\SL[\mathbf{a}\times \mathbf{b}] = [\SL[\mathbf{a}], \SL[\mathbf{b}]] = \SL[\mathbf{a}]\SL[\mathbf{b}] - \SL[\mathbf{b}]\SL[\mathbf{a}]$, which shows that $\SL$ is an isomorphism between $\mathbb{R}^3$ and $\mathfrak{so}(3)$.
\end{example}
\begin{example}
    The Lie algebra of the Euclidean group $\text{E}(n)$ and the special Euclidean group $\text{SE}(n)$ is denoted $\mathfrak{se}(n)$, and is the set 
    \begin{align*}
        \mathfrak{se}(n) = \left\{\begin{bmatrix}
            \mathbf{A} & \mathbf{b} \\ \mathbf{0} & 0
        \end{bmatrix} \in \mathbb{R}^{(n+1)\times(n+1)} | \mathbf{A}\in\mathfrak{so}(n),\, \mathbf{b}\in\mathbb{R}^n\right\}.
    \end{align*}
    specially in the case of $\mathfrak{se}(3)$, although some texts refer to elements of the Lie algebra as twists, the most common representation is given by a six-dimensional vector, the twit, $\boldsymbol{\zeta} = [\zeta_1\ \zeta_2\ \zeta_3\ \zeta_4\ \zeta_5\ \zeta_6]^\top$ and an isomorphism $\SL$ is given by
    \begin{align*}
        \SL[\boldsymbol{\zeta}] = \begin{bmatrix}
            0 & -\zeta_6 & \zeta_5 & \zeta_1 \\
            \zeta_6 & 0 & -\zeta_4 & \zeta_2 \\
            -\zeta_5 & \zeta_4 & 0 & \zeta_3 \\
            0 & 0 & 0 & 0
        \end{bmatrix},
    \end{align*}
    where $[\zeta_1\ \zeta_2\ \zeta_3]^\top$ is the linear velocity $\mathbf{v}$ and $[\zeta_4\ \zeta_5\ \zeta_6]^\top$ is the angular velocity $\boldsymbol{\omega}$.
\end{example}
\begin{example}
    The Lie algebra of $\text{T}(n)$ is a subset of $\mathfrak{se}(n)$, and is given by
    \begin{align*}
        \mathfrak{t}(n) = \left\{\begin{bmatrix}
            \mathbf{0} & \mathbf{a} \\ \mathbf{0} & 0
        \end{bmatrix} \in \mathfrak{se}(n) | \mathbf{a}\in\mathbb{R}^n\right\}.
    \end{align*} 
\end{example}
\begin{example}
    The Lie algebra of the independent special Euclidean group $\text{ISE}(n)$ is given by
    \begin{align*}
        \mathfrak{ise}(n) &= \left\{\begin{bmatrix}
            \mathbf{A} & \mathbf{0}\\
            \mathbf{0} & \mathbf{B}
        \end{bmatrix} \in \mathbb{R}^{(2n+1)\times(2n+1)}\,;\, \mathbf{A}\in\mathfrak{so}(n),\,\mathbf{B}\in\mathfrak{t}(n)\right\}\\
        &= \left\{\begin{bmatrix}
            \mathbf{A} & \mathbf{0} & \mathbf{0}\\
            \mathbf{0} & \mathbf{0} & \mathbf{b}\\
            \mathbf{0} & \mathbf{0} & 0
        \end{bmatrix} \in \mathbb{R}^{(2n+1)\times(2n+1)}\,;\, \mathbf{A}\in\mathfrak{so}(n),\,\mathbf{b}\in\mathbb{R}^n\right\}.
    \end{align*}
    This means that $\mathfrak{ise}(n) = \mathfrak{so}(n)\oplus\mathfrak{t}(n)$, as expected from the definition of the Lie algebra of a direct product of Lie groups.
\end{example}
\begin{example}
    The symplectic Lie algebra $\mathfrak{sp}(2n, \mathbb{R})$ is defined as
    \begin{align*}
        \begin{split}
            \mathfrak{sp}(2n, \mathbb{R}) &= \left\{ \mathbf{A} \in \mathbb{R}^{2n\times 2n} | \mathbf{A}^T\boldsymbol{\Omega} + \boldsymbol{\Omega}\mathbf{A} = 0 \right\} \\&= \left\{ \begin{bmatrix} \mathbf{B} & \mathbf{C} \\ \mathbf{D} & -\mathbf{B}^T \end{bmatrix}\in \mathbb{R}^{2n\times 2n}| \mathbf{C} = \mathbf{C}^\top,\, \mathbf{D} = \mathbf{D}^\top\right\},            
        \end{split}
    \end{align*}
    where $\boldsymbol{\Omega}$ the same as defined in \autoref{ex:symplectic-group}. Clearly, the dimension of $\mathfrak{sp}(2n, \mathbb{R})$ is $n(2n + 1)$. Thus,
    for a vector $\boldsymbol{\zeta} = [\zeta_1\ \dots\ \zeta_{2n^2+n}]^\top$, an isomorphism $\SL$ is given by
    \begin{align*}
        \SL[\boldsymbol{\zeta}] = \begin{bmatrix}
            \zeta_1 & \dots & \zeta_n & \zeta_{n^2 + 1} & \dots & \zeta_{n^2 + n} \\
            \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
            \zeta_{n^2 - n + 1} & \dots & \zeta_{n^2} & \zeta_{n^2 + n} & \dots & \zeta_{\frac{3n^2+n}{2}}\\
            \zeta_{\frac{3n^2+n}{2} + 1} & \dots & \zeta_{\frac{3n^2+3n}{2}} & -\zeta_1 & \dots & -\zeta_{n^2 - n + 1}\\
            \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
            \zeta_{\frac{3n^2+3n}{2}} & \dots & \zeta_{2n^2 + n}& -\zeta_n & \dots & -\zeta_{n^2} 
        \end{bmatrix},
    \end{align*}
\end{example}
\begin{example}
    The Lie algebra of the indefinite orthogonal group $\text{O}(p, q)$ and the indefinite special orthogonal group $\text{SO}(p, q)$ are the same and denoted $\mathfrak{so}(p, q)$. This Lie algebra is defined by
    \begin{align*}
        \mathfrak{so}(p, q) = \left\{
            \begin{bmatrix}
                \mathbf{A} & \mathbf{B}\\
                \mathbf{B}^\top & \mathbf{C}
            \end{bmatrix} \in \mathbb{R}^{(p+q)\times(p+q)} | \mathbf{A}^\top = -\mathbf{A},\, \mathbf{C}^\top = -\mathbf{C},\, \mathbf{B} \in \mathbb{R}^{p\times q}
        \right\},
    \end{align*}

    As for the most common subgroups: the full Lorentz group $\text{O}(3, 1)$, the special Lorentz group $\text{SO}(3, 1)$, and the proper orthochronous Lorentz group $\text{SO}^+(3, 1)$ share the same Lie algebra $\mathfrak{so}(3, 1)$, for which an isomosphism for a vector $\boldsymbol{\zeta} = [\zeta_1\ \zeta_2\ \zeta_3\ \zeta_4\ \zeta_5\ \zeta_6]^\top$ is described as follows:
    \begin{align*}
        \SL[\boldsymbol{\zeta}] = \begin{bmatrix}
            0 & \zeta_1 & \zeta_2 & \zeta_4 \\
            -\zeta_1 & 0 & \zeta_3 & \zeta_5 \\
            -\zeta_2 & -\zeta_3 & 0 & \zeta_6 \\
            \zeta_4 & \zeta_5 & \zeta_6 & 0
        \end{bmatrix}.
    \end{align*}
\end{example}
\subsection{Exponential Map}
The \emph{exponential map} is a fundamental concept in the study of Lie groups and Lie algebras, as it provides a way to map elements of the Lie algebra to elements of the Lie group. Although it is defined for any Lie group, we will focus on matrix Lie groups, as the definition will be more straightforward and related to our work. The exponential is simply given by the matrix exponential, defined as
\begin{align}
    \exp : \mathfrak{g} &\to G,
\end{align}
and it is given by the power series
\begin{align}
    \exp(\mathbf{A}) = \sum_{k=0}^\infty \frac{\mathbf{A}^k}{k!}.
\end{align}

The exponential map is also useful for obtaining the Lie algebra of a Lie group, as the following theorem shows.
\begin{theorem}[Von Neumann and Cartan, 1927]
    Let $G\subset\text{GL}(n, \mathbb{R})$ be a matrix Lie group. The set $\mathfrak{g}$ defined as
    \begin{align}
        \mathfrak{g} = \left\{\mathbf{A}\in\mathbb{R}^{n\times n} | \exp(\sigma\mathbf{A})\in G\,\forall\,\sigma\in\mathbb{R}\right\}
    \end{align}
    is a vector space equal to the tangent space $T_\iota G$ at the identity $\iota$. Furthermore, $\mathfrak{g}$ is closed under the Lie bracket $[\mathbf{A}, \mathbf{B}] = \mathbf{A}\mathbf{B} - \mathbf{B}\mathbf{A}\,\forall\,\mathbf{A},\mathbf{B}\in\mathbb{R}^{n\times n}$.\hfill\qedsymbol
\end{theorem}

Although the exponential map maps elements of the Lie algebra to the Lie group, it is not necessarily surjective. All elements $\exp(\mathbf{A})$ lie in the identity component $G_\iota$ of the Lie group, and the image of the exponential map is a neighborhood of the identity element in $G_\iota$ \citep[p. 56]{Hall2015}. Although there are sufficient conditions for the exponential map to be surjective, for example, if the Lie group is connected and compact \citep[p. 316]{Hall2015}, there is no necessary conditions for this property in the literature. Groups that possess this property are denoted \emph{exponential Lie groups} \citep{djokovic1995exponential}, for which examples are the special orthogonal group $\text{SO}(n)$ \citep[p. 28]{Gallier2020}, the special Euclidean  group $\text{SE}(n)$ \citep[p. 42]{Gallier2020}, and the Heisenberg group $\text{H}$ \citep[p. 75]{Hall2015}. Additional non-trivial examples can be found in \citet{djokovic1995exponential}. In contrast, examples of non-exponential Lie groups include the special linear group $\text{SL}(n)$ \citep[p. 28]{Gallier2020}.

\subsection{Start of Automatica portion}
In this section, we recall several properties of Lie groups and Lie algebras that will be essential for developing our extension. First, a Lie group $G$ is a smooth manifold equipped with a group structure. The Lie algebra $\mathfrak{g}$ associated with $G$ has two equivalent definitions, either of which we will use as appropriate. One definition describes the Lie algebra as the tangent space at the group identity $G_e$ \citep[p. 16]{Gallier2020}. Alternatively, a Lie algebra can be viewed as the space of all smooth vector fields on the Lie group, under the Lie bracket operation on vector fields \citep[p. 190]{Lee2012}. As said previously, we will consider connected matrix Lie groups, and henceforth we will denote by $n$ the dimension of any (square) matrix in a given group, and the dimension of the group by $m$, which is the same as the dimension of the associated Lie algebra.

\begin{lemma}\label{lemma:lie-group-flow}
    (Adapted from \citet[p. 570]{Gallier2020}) Given a Lie group $G$, if $V$ be a right-invariant (resp. left-invariant) vector field, $\theta:\mathbb{R}\times G$ its global flow, and $\gamma_\mathbf{X}=\theta(t, \mathbf{X})$ the associated maximal integral curve with initial condition $\mathbf{X}\in G$, then
    \begin{align*}
        \gamma_\mathbf{X}(t) = \theta(t, \mathbf{X}) &=  \theta(t, G_e)\mathbf{X}\\
        \bigl(\text{resp. }\theta(t, \mathbf{X}) &=  \mathbf{X}\theta(t, G_e)\bigr)
    \end{align*}
\end{lemma}
\begin{proof}
    Let $\gamma(t) = R_{\mathbf{X}}\theta(t, G_e)$, then $\gamma(0) = \mathbf{X}$. Applying the chain rule:
    \begin{align}
        \dot{\gamma}(t) = d(R_{\mathbf{X}})_{\theta(t, G_e)}\Bigl(V\bigl(\theta(t, G_e)\bigr)\Bigr) = V\Bigl(R_{\mathbf{X}}\bigl(\theta(t, G_e)\bigr)\Bigr) = V(\gamma(t)).
    \end{align}
    Since maximal integral curves are unique, it follows that $\gamma(t) = \theta(t, \mathbf{X})$, thus $\theta(t, \mathbf{X}) = \theta(t, G_e)\mathbf{X}$. The proof for the left-invariant case is analogous.
\end{proof}

Next, we present a fact that will be important for the forthcoming analysis. 
\begin{lemma} \label{lemma:derivative-lie-element-H-parallelizable} Let $\mathbf{G}:\mathbb{R}\to G$ be a differentiable function. Then, there exists a function $\mathbf{A}:\mathbb{R} \to \mathfrak{g}$ such that 
\begin{align}
    \frac{d}{d\sigma} \mathbf{G}(\sigma) = \mathbf{A}(\sigma) \mathbf{G}(\sigma). \label{eq:derivative-lie-element-H-parallelizable}
\end{align}

\end{lemma}
\begin{proof}
    Let $\mathbf{G}(\sigma)$ be the maximal integral curve associated with a global flow $\theta(\sigma, \mathbf{G}_0)$ of a right-invariant vector field $V$ on a Lie group $G$. By \cref{lemma:lie-group-flow}, we can write
    \begin{align}
        \frac{d}{d\sigma} \mathbf{G}(\sigma) = d(R_{\mathbf{G}(\sigma)})_{\theta(\sigma, G_e)} \Bigl(V\bigl(\theta(\sigma, G_e)\bigr)\Bigr). \label{eq:proof-derivative-lie-element-H-parallelizable-part1}
    \end{align}
    
    Since the vector space of right-invariant vector fields, denoted by $\mathfrak{g}^R$, is isomorphic (more precisely, anti-isomorphic) to the Lie algebra $\mathfrak{g}$ \citep[p. 569]{Gallier2020}, this implies that every basis for $\mathfrak{g}^R$ is a right-invariant global frame for $G$ (and consequently every Lie group is parallelizable), which in turn allows us to write $V\bigl(\theta(\sigma, G_e)\bigr) = \mathbf{A}(\sigma)$ for some $\mathbf{A}(\sigma)$ in $\mathfrak{g}$.

    Now, since in our case $R_\mathbf{X}$ is the restriction to $\text{GL}(n, \mathbb{R})$ of the linear map $\mathbf{Y}\mapsto \mathbf{Y}\mathbf{X}$, its differential can be expressed as $dR_\mathbf{X}(W) = W\mathbf{X}$ \citep[p. 194]{Lee2012}. Thus, we can write \eqref{eq:proof-derivative-lie-element-H-parallelizable-part1} as
    \begin{align}
        \frac{d}{d\sigma} \mathbf{G}(\sigma) = d(R_{\mathbf{G}(\sigma)})_{\theta(\sigma, G_e)} \bigl(\mathbf{A}(\sigma)\bigr) = \mathbf{A}(\sigma)\mathbf{G}(\sigma).
    \end{align}
    % It is a known fact that Lie groups are parallelizable using right-invariant vector fields as a basis \citep{GRIGORIAN2024804}. That means that any vector on the tangent space on any point $\mathbf{G} \in G$ can be written as $\mathbf{AG}$ in which $\mathbf{A}$ is an element of the Lie algebra $\mathfrak{g}$ (possibly different for each $\mathbf{G}$). This implies the desired result.
\end{proof}

 On the other hand, since the Lie algebra is a vector space, we can define a basis $\{\mathbf{E}_k\}\in\mathfrak{g}, k\in[1,m]$. Given a basis, for any $\mathbf{A} \in\mathfrak{g}$, there exists scalars $\{\zeta_k\},\, k\in[1,m]$, such that $\mathbf{A} = \sum_{k=1}^{m} \zeta_k\mathbf{E}_k$. Furthermore, for each choice of basis for the Lie algebra, it becomes possible to define uniquely a respective linear operator $\SL:\mathbb{R}^{m}\to\mathfrak{g}$ as follows:
\begin{definition}[S map]\label{def:SL-left-isomorphism-act-on-xi}
    Let $\mathbf{E}_1,\dots,\mathbf{E}_m$ be a basis for an $m$-dimensional Lie algebra $\mathfrak{g}$, and $\boldsymbol{\zeta}$ an $m$-dimensional vector. Then, there exists a unique isomorphism $\SL:\mathbb{R}^{m}\to\mathfrak{g}$ defined as
    \begin{equation}
        \SL[\boldsymbol{\zeta}] \triangleq \sum_{k=1}^m\zeta_k\mathbf{E}_k.    
    \end{equation}
    
\end{definition}
Note that $\SL$ is indeed a \emph{linear operator}. Some common examples of isomorphisms are as following.


Considering \autoref{lemma:derivative-lie-element-H-parallelizable} and \autoref{def:SL-left-isomorphism-act-on-xi} we can conclude the following important fact.

\begin{lemma} \label{lemma:very-important-fact}
    Given a differentiable function $\mathbf{\mathbf{G}}:\mathbb{R}\to G$, there exists a function $\boldsymbol{\zeta}:\mathbb{R}\to\mathbb{R}^m$, such that
    \begin{align}
    \label{eq:importantresult}
    \frac{d}{d\sigma} \mathbf{\mathbf{G}}(\sigma)=\SL\bigl(\boldsymbol{\zeta}(\sigma)\bigr)\mathbf{\mathbf{G}}(\sigma). 
\end{align}

\end{lemma}
\begin{proof} This is a direct consequence of \cref{lemma:derivative-lie-element-H-parallelizable} and \cref{def:SL-left-isomorphism-act-on-xi}. 
\end{proof}

Since $\SL$ is an isomorphism, the \emph{inverse map} that maps an element of the Lie algebra to a vector can be defined as well:
\begin{definition}[Inverse S map]\label{def:inverse-isomorphism-SLinv}
    Let $\mathfrak{g}$ be an $m$-dimensional Lie algebra. The \emph{inverse map} is defined as $\invSL:\mathfrak{g}\to\mathbb{R}^m$, such that $\invSL[\SL[\boldsymbol{\zeta}]] = \boldsymbol{\zeta}$. 
\end{definition}

\section{Adaptive Control}\label{sec:background-adaptive-control}
Adaptive control emerges in 1950 as a response to the design of aircraft autopilots. Aircrafts operate on a wide range of speeds and altitudes, and thus its parameters suffers great variations. This inspired the main idea of adaptive control, on-line estimation of parameters based on measured system signals \citep{Slotine1991}. This need is also present for other systems: robot manipulators may need to carry objects of unknown mass; chemical processes may have unmodeled dynamics; the fuel consumption of a vehicle might pose a challenge for common controllers. In this section, we will briefly present the main concepts of adaptive control, basing our discussion on the books \citet{Slotine1991}, \citet{Krstic1995} and \citet{Ioannou2012}.

An adaptive controller is formed by the combination of a control law, designed for a nominal model, and an adaptation law, which is responsible for estimating the unkown parameters of the plant. The way the parameters estimator is designed divides the adaptive control into two main categories: \emph{direct} and \emph{indirect} adaptive control. In indirect adaptive control, the plant parameters are estimated on-line and are used to calculate the controller parameters. In the direct approach, the plant model is parameterized in terms of the controller parameters, and these are the parameters that get estimated on-line.

More specifically, in indirect approach, the plant model $P(\theta)$ is parameterized with respect to some unkown parameter vector $\theta$. For example, using the Euler-Lagrange for dynamic systems, the plant would be represented by a regressor matrix $\mathbf{Y}$ and a parameter vector $\theta$, that contains the unknowns. The adaptation law performs an on-line estimation of the parameters $\widehat{\theta}(t)$, and one obtains a estimated plant $\widehat{P}(\widehat{\theta})$. This plant treated as the true model, and is used in the design of the controller: the controller parameters $\theta_c(t)$ are obtained by solving some equation dependent on the estimated plant. A block diagram of this approach is shown in \cref{fig:indirect-adaptive-control}.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[
        block/.style={draw, rectangle, minimum height=1.2cm, minimum width=2.4cm, align=center},
        arrow/.style={->, >=stealth, thick},
        label/.style={font=\small}
    ]
    
    % Nodes
    \node[block] (controller) {Controller \\ $C(\theta_c(t))$};
    \node[block, right=2cm of controller] (plant) {Plant \\ $P(\theta)$};
    \node[block, below=1cm of plant] (estimation) {Estimation of $\theta$};
    \node[block, below=1.5cm of estimation] (calculation) {Calculations \\ $\theta_c(t) = F(\widehat{P}(\widehat{\theta}(t)))$};
    
    % Arrows between blocks
    \draw[arrow] (controller) -- node[above, label] {$u$} (plant);
    \draw[arrow] (plant) -- (estimation);
    % \draw[arrow] (plant) |- node[right, label] {$r$} (estimation);
    \draw[arrow] (estimation) -- node[right, label] {$\widehat{\theta}(t)$} (calculation);
    \draw[arrow] (calculation) -| node[left, label] {$\theta_c(t)$} (controller);
    
    % Input and Output Arrows
    \node[left=1.5cm of controller] (input) {Input $r$};
    \draw[arrow] (input) -- (controller.west);
    
    \node[right=0.75cm of estimation] (rr) {$r$};
    \draw[arrow] (rr) -- (estimation.east);

    % \node[right=0.9cm of controller] (csign) {};
    \draw[arrow] ($(controller.east)+(1,0)$) |- (estimation.west);
    
    \node[right=1.5cm of plant] (output) {$y$};
    \draw[arrow] (plant.east) -- (output);
    
    % Feedback loop
    \draw[arrow] ($(plant.east)+(0.75,0)$) |- ++(0,1.5) -| (controller.north);
    \draw[arrow] ($(plant.east)+(0.75,0)$) |- node[left, label] {} ($(estimation.east)+(0, 0.25)$);
    
    \end{tikzpicture}
    \caption{Block diagram of an indirect adaptive control system.}
    \label{fig:indirect-adaptive-control}
\end{figure}

In direct adaptive control, the plant model $P(\theta)$ is parameterized in terms of the unkown controller parameter vector $\theta_c$, such that this parametrization with the true controller parameters $P_c(\theta_c)$ renders the same results as the true plant. This parametrization is used to estimate the values of the controller parameters $\widehat{\theta}_c(t)$, which are used to calculate the control signal. A block diagram of this approach is shown in \cref{fig:direct-adaptive-control}.
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[
        block/.style={draw, rectangle, minimum height=1.2cm, minimum width=2.4cm, align=center},
        arrow/.style={->, >=stealth, thick},
        label/.style={font=\small}
    ]
    
    % Nodes
    \node[block] (controller) {Controller \\ $C(\widehat{\theta}_c)$};
    \node[block, right=2cm of controller] (plant) {Plant \\ $P(\theta)\to P_c(\theta_c)$};
    \node[block, below=1cm of plant] (estimation) {Estimation of $\theta_c$};
    
    % Arrows between blocks
    \draw[arrow] (controller) -- node[above, label] {$u$} (plant);
    \draw[arrow] (plant) -- (estimation);
    \draw[arrow] (estimation.south) |- ++(0, -0.5) -| node[left, label] {$\widehat{\theta}_c$} (controller);
    
    % Input and Output Arrows
    \node[left=1.5cm of controller] (input) {Input $r$};
    \draw[arrow] (input) -- (controller.west);
    
    \node[right=0.75cm of estimation] (rr) {$r$};
    \draw[arrow] (rr) -- (estimation.east);

    % \node[right=0.9cm of controller] (csign) {};
    \draw[arrow] ($(controller.east)+(1,0)$) |- (estimation.west);
    
    \node[right=1.5cm of plant] (output) {$y$};
    \draw[arrow] (plant.east) -- (output);
    
    % Feedback loop
    \draw[arrow] ($(plant.east)+(0.75,0)$) |- ++(0,1.5) -| (controller.north);
    \draw[arrow] ($(plant.east)+(0.75,0)$) |- node[left, label] {} ($(estimation.east)+(0, 0.25)$);
    
    \end{tikzpicture}
    \caption{Block diagram of a direct adaptive control system.}
    \label{fig:direct-adaptive-control}
\end{figure}

A common usage of the indirect approach is in Model Reference Adaptive Control (MRAC), in which the plant is compared to a reference model, and the controller is designed to make the plant behave like the reference model. This reference model is designed such that it describes the desired relationship between the input and output of the plant. The controller is then designed to change the dynamics of the plant to match the reference model. This is the approach that will be used in this text.

One common problem with adaptive control is the \emph{parameter drift}. This occurs when the input signal is not persistently exciting, that is, the input signal does not contain enough information to estimate the parameters. This can lead to the parameters drifting away from the true values, and the control system becoming unstable, even though it might initially seem stable for some interval. This is a common problem in practice, and several methods have been proposed to mitigate this issue.

The most common and simple technique is the \emph{dead-zone} or \emph{deadband}. This technique consists of setting a threshold for the parameter adaptation, such that if the tracking error is below this threshold, the adaptation dynamics are set to $0$. For example, if the adaptation law is described by $\dot{\widehat{\boldsymbol{\theta}}} = -\gamma\mathbf{Y}\mathbf{e}$, then the dead-zone implies
\begin{align*}
    \dot{\widehat{\boldsymbol{\theta}}} = \begin{cases}
        -\gamma\mathbf{Y}\mathbf{e} &\|\mathbf{e}\| > \epsilon,\\
        \mathbf{0} & \|\mathbf{e}\| \le \epsilon,
    \end{cases}
\end{align*}
where $\epsilon$ is the size of the dead-zone, or deadband.

Many common other techniques reside under the same branch of \emph{leakage} modification, which is based on Lyapunov analysis. These techniques have the same format $\dot{\widehat{\boldsymbol{\theta}}} = -\gamma\mathbf{Y}\mathbf{e} - \gamma w\widehat{\boldsymbol{\theta}}$, where $w(t)$ is the \emph{leakage term}. The most common leakage term is the \emph{$\sigma$-modification}, that consists of setting a positive small constant $\sigma$ as the leakage term. If one combines the dead-zone with the $\sigma$-modification, the result is the \emph{switching $\sigma$} technique, where $w(t)=\sigma$, with $\sigma=\sigma_0$ if $\|\widehat{\boldsymbol{\theta}}\|\ge M$, or $\sigma=\mathbf{0}$ otherwise. The \emph{$\epsilon$-modification} consists of using the norm of the tracking error in the leakage term: $w(t) = \nu_0\|\mathbf{e}\|$.

There are many other techniques to improve robustness, as covered in \citet[ch. 8]{Ioannou2012}, such as \emph{parameter projection}, which constrains the parameter estimates within predefined bounds; \emph{normalization}, which mitigates large parameter variation. With these techniques, and a choice of adaptation law, we can design an adaptive controller that is suitable for application.