% !TeX root = main.tex
\chapter{Thoeretical Background}\label{ch:background}
In this chapter we revisit the vector field strategy in the euclidean space based on a curve parametrization \citep{Rezende2022}, as it will prove valuable in drawing connections between both works. We also present some fundamental concepts of Lie groups and Lie algebras, which will be essential for the development of our extension.

\section{Vector Field in Euclidean Space}\label{sec:adriano-review}
For clarity, we revisit the vector field strategy presented in \citet{Rezende2022}. Since our work extends this previous approach, this review will help establish direct connections between both works and highlight the core aspects of our contributions. The primary goal of the authors in \citet{Rezende2022} is to develop an artificial $n$-dimensional vector field that guides system trajectories toward a predefined curve and ensures circulation around it. A key element of this formulation is the definition of a distance function with essential properties. As their work focuses solely on Euclidean space, they adopt the Euclidean distance for the vector field computation, which is derived using a parametric representation of the curve.

We summarize the main steps for constructing this vector field, emphasizing the most critical properties. Although the original paper addresses time-varying curves, we limit our discussion to the static portion of the methodology. The authors consider a system modeled as the following single integrator 
\begin{align}
    \dot{\mathbf{h}} = \boldsymbol{\xi}, \label{eq:adriano-single-integrator}
\end{align}
where $\mathbf{h}\in\mathbb{R}^m$ represents the system state, and $\boldsymbol{\xi}\in\mathbb{R}^m$ denotes the system input. The objective is to compute a vector field $\Psi:\mathbb{R}^m\to\mathbb{R}^m$ such that, if the system input is equal to the vector field, the system trajectories converge to and follow the target curve $\mathcal{C}$, for which a parametrization is given by $\mathbf{h}_d(s)$. Despite relying on a parametric representation of the curve, it is important to note that the resulting computations are independent of the specific parametrization chosen.

The authors define their distance function $D$ as the Euclidean distance between the system's current state and the nearest point on the curve, i.e., 
\begin{align}
    D(\mathbf{h}) \triangleq \min_{s}\widehat{D}(\mathbf{h}, \mathbf{h}_d(s))=\min_{s}\|\mathbf{h}- \mathbf{h}_d(s)\|. \label{eq:adriano-EC-distance}   
\end{align}
In this context, we denote $s^*$ as the optimal parameter such that $\mathbf{h}_d(s^*(\mathbf{h}))$ is the closest point on the curve to the current state.
\begin{figure}
    \centering
    \def\svgwidth{.8\linewidth}
    \import{figures/}{plotly_vf2.pdf_tex}
    \caption{Example showing the vector field and the components for a point $\mathbf{h}\in G$ and curve $\mathcal{C}$.}
    \label{fig:vector-field-adriano}
\end{figure}

Next, the authors introduce two components of their vector field: the \emph{normal} component $\boldsymbol{\xi}_{N}$, responsible for convergence, and the \emph{tangent} component $\boldsymbol{\xi}_{T}$, which ensures circulation. The resulting expression for the vector field is 
\begin{align}
    \Psi(\mathbf{h}) = k_N(\mathbf{h})\boldsymbol{\xi}_{N}(\mathbf{h}) + k_T(\mathbf{h})\boldsymbol{\xi}_{T}(\mathbf{h}), \label{eq:adriano-vector-field-expression}    
\end{align}
where $k_N$ and $k_T$ are gains, dependent on the system state, that balance the predominance of the normal and tangent components. This vector field strategy is illustrated in \autoref{fig:vector-field-adriano}. The normal component, $\boldsymbol{\xi}_{N}$, is naturally taken as the negative gradient of the distance function, due to the use of Euclidean distance:
\begin{align}
    \boldsymbol{\xi}_{N} = -\nabla D.
\end{align}

There is a key aspect of the normal component that is crucial for the convergence proof using Lyapunov stability theory: the fact that the time derivative of the distance function can be expressed as $\dot{D}=-\boldsymbol{\xi}_{N}^{\top}{\boldsymbol{\xi}}$. We emphasize the significance of this feature, as it will play an important role in our extension. In our approach, the normal component is similarly constructed by identifying the term that arises when differentiating the distance function.

Next, we address the tangent component. This component is solely related to the target curve and is defined as the tangent vector at the nearest point on the curve, i.e.,
\begin{align}
    \boldsymbol{\xi}_{T}(\mathbf{h}) = \frac{d}{ds}\mathbf{h}_d(s)|_{s=s^*(\mathbf{h})}.
\end{align}
A noteworthy property of both components is that they are orthogonal to each other, which is essential in the proof of convergence for this algorithm.

In the Lyapunov stability proof, the final result shows that the time derivative of $D$ is negative semidefinite. The proof is then completed using two other essential properties: the fact that the distance function has no local minima outside the curve, and the fact that the ``generalized gradient'' of this function never vanishes. With these features, the authors demonstrate that if the system trajectories follow the vector field, the system will converge to and circulate around a predefined curve. A summary of these key features is as follows:
\begin{feature}
    \item The time derivative of the distance function is the opposite of the dot product between the so called \emph{normal} component and the system control input; \label{feat:adriano-time-derivative-lyapunov-normal-comp}
    \item The \emph{normal} and \emph{tangent} components are orthogonal to each other; \label{feat:adriano-orthogonality}
    \item The distance function has no local minima outside the target curve. Furthermore, whenever the distance function is differentiable, its gradient never vanishes. \label{feat:adriano-no-local-minima}
\end{feature}
In our generalization, we will incorporate and build upon these features.

\section{Lie groups and Lie algebras}
In this section, we introduce \emph{Lie groups} and \emph{Lie algebras}, which are smooth manifolds that are also groups. Besides providing examples of both, we also introduce concepts that will prove valuable in our work. We begin with the general definition of a Lie group.
\subsection{Lie groups}
\begin{definition}[Lie group]
    A \emph{Lie group} is a smooth manifold $G$ satisfying the following properties:
    \begin{property}
        \item $G$ is a group (with identity element denoted $\iota$)
        \item $G$ is a topological group (the group operation and the inverse map are continuous). Furthermore, the group operation $\circ: G\times G\to G$ and the inverse map $\cdot^{-1}: G\to G$ are smooth.
    \end{property}
\end{definition}

The mapping between Lie groups is a fundamental concept and will play an important role in our extension, specifically for drawing connections between our extension and the vector field strategy in Euclidean space. Usually these mappings have additional properties and are defined as follows:
\begin{definition}
    If $G$ and $H$ are Lie groups, a \emph{homomorphism} $\phi: G\to H$ is a smooth map (between manifolds $G$ and $H$) that is also a group homomorphism, that is, if $\circ$ and $\star$ are the group operations in $G$ and $H$, respectively, then $\phi(\mathbf{X}\circ\mathbf{Y}) = \phi(\mathbf{X})\star\phi(\mathbf{Y})$ for all $\mathbf{X},\mathbf{Y}\in G$. If this map is also a diffeomorphism (the inverse map $\phi^{-1}$ is also a homomorphism), then $\phi$ is an \emph{isomorphism}. In this case, we say that $G$ and $H$ are \emph{isomorphic} and denote this by $G\cong H$.
\end{definition}

There are two important isomorphisms in Lie groups that are important for the concept of Lie algebras. These are the translation isomorphisms, which are defined as follows:
\begin{definition}
    Given a Lie group $G$, we define two operations: the left translation $L_{\mathbf{X}}: G\to G$ and the right translation $R_{\mathbf{X}}: G\to G$ for any $\mathbf{X}\in G$. These operations are defined as 
    \begin{align}
        L_{\mathbf{X}}(\mathbf{Y}) &= \mathbf{X}\circ\mathbf{Y}\\
        R_{\mathbf{X}}(\mathbf{Y}) &= \mathbf{Y}\circ\mathbf{X},
    \end{align}
    respectively. Since the group operation and the inverse map are smooth, the left and right translations are diffeomorphisms, and thus isomorphisms.
\end{definition}

\subsubsection{Matrix Lie Groups}
In general lines, Lie groups that can be represented by matrices are called \emph{matrix Lie groups}. These groups are defined by a set of matrices that satisfy the group properties. The group operation is matrix multiplication, the identity element is the identity matrix, and the inverse map is the matrix inverse. For a more precise definition, we need to define the \emph{general linear group} and the concept of \emph{convergence} of a sequence of matrices.
\begin{definition}
    The \emph{general linear group} over the real numbers, denoted $\text{GL}(n, \mathbb{R})$, is the set of all invertible $n\times n$ matrices with real entries. The general linaer group over the complex numbers, denoted $\text{GL}(n, \mathbb{C})$, is the group of all invertible $n\times n$ matrices with complex entries.
\end{definition}
\begin{definition}
    Let $\mathbf{X}_k$ be a sequence of complex matrices in $\mathbb{C}^{n\times n}$. We say that $\mathbf{X}_k$ \emph{converges} to a matrix $\mathbf{Y}$ if each entry of $\mathbf{X}_k$ converges to the corresponding entry of $\mathbf{Y}$ as $k\to\infty$. This means that for each $i,j\in[1,n]$, the sequence $\{\mathbf{X}_k\}_{ij}$ converges to $\mathbf{Y}_{ij}$.
\end{definition}
With both definitions, we can now define a matrix Lie group formally.
\begin{definition}
    A \emph{matrix Lie group} is a subgroup $G$ of $\text{GL}(n, \mathbb{C})$ with the property that if $\mathbf{X}_k$ is a sequence of matrices in $G$, and $\mathbf{X}_k$ converges to some matrix $\mathbf{Y}$, then either $\mathbf{Y}$ is in $G$ or $\mathbf{Y}$ is not invertible. This property is equivalent to sayint that $G$ is a \emph{closed subgroup} of $\text{GL}(n, \mathbb{C})$.
\end{definition}

Although present in the context of any Lie group, we can define two interesting properties more easily for matrix Lie groups. The knowledge of \emph{compactness} and \emph{connectedness} can reduce complexity in the study of these groups.
\begin{definition}
    A matrix Lie group $G\subset\text{GL}(n,\mathbb{C})$ is said to be \emph{compact} if and only if:
    \begin{property}
        \item whenever a sequence $\mathbf{X}_k$ is in $G$ and $\mathbf{X}_k$ converges to $\mathbf{Y}$, then $\mathbf{Y}$ is in $G$;
        \item there exists a constant $C>0$ such that for all $\mathbf{X}\in G$, $\|\{\mathbf{X}\}_{ij}\|\leq C$ for all $i,j\in[1,n]$.
    \end{property}
\end{definition}
\begin{definition}
    A matrix Lie group $G$ is termed \emph{connected} if for all $\mathbf{X},\mathbf{Y}\in G$, there exists a continuous path $\mathbf{G}(\sigma)\in G$, $a\le\sigma\le b$ such that $\mathbf{G}(a) = \mathbf{X}$ and $\mathbf{G}(b) = \mathbf{Y}$. The \emph{identity component} of $G$, denoted $G_\iota$, is the set $\mathbf{X}\in G$ for which there exists a continuous path $\mathbf{G}(\sigma)\in G$, $a\le\sigma\le b$ such that $\mathbf{G}(a) = \iota=\mathbf{I}$ and $\mathbf{G}(b) = \mathbf{X}$.
\end{definition}

\subsubsection{Examples of Lie groups}
\begin{example}\label{ex:general-linear-group-special-linear-group}
    The \emph{special linear group} $\text{SL}(n, \mathbb{R})$ (resp. $\text{SL}(n, \mathbb{C})$) is a subgroup of $\text{GL}(n, \mathbb{R})$ (resp. $\text{GL}(n, \mathbb{C})$) consisting of invertible matrices with determinant equal to 1.
\end{example}
\begin{example}\label{ex:orthogonal-group-special-orthogonal-group}
    The \emph{orthogonal group} $\text{O}(n)$ is the Lie group of distance-preserving transformation and comprises rotations and reflections. It consists of all $n\times n$ orthogonal matrices:
    \begin{align}
        \text{O}(n) = \left\{\mathbf{X}\in\mathbb{R}^{n\times n} | \mathbf{X}^T\mathbf{X} = \mathbf{I}_n\right\}.
    \end{align}

    Its subgroup $\text{SO}(n)$, the \emph{special orthogonal group}, consists of all orthogonal matrices with determinant equal to 1. This group represents rotations and as such are formed by the set of rotation matrices:
    \begin{align}
        \text{SO}(n) = \left\{\mathbf{X}\in\mathbb{R}^{n\times n} | \mathbf{X}^T\mathbf{X} = \mathbf{I}_n,\, \det(\mathbf{X}) = 1\right\}.
    \end{align}
\end{example}
\begin{example}\label{ex:euclidean-group-special-euclidean-group}
    The \emph{Euclidean group} $\text{E}(n)$ is a Lie group of isometries in Euclidean space. This group is formed by the semidirect product $\mathbb{R}^n \rtimes \text{O}(n)$ and is defined as
    \begin{align}
        \text{E}(n) = \left\{\begin{bmatrix}
            \mathbf{R} & \mathbf{p} \\ \mathbf{0} & 1
        \end{bmatrix} \in \mathbb{R}^{(n+1)\times(n+1)} | \mathbf{R}\in\text{O}(n),\, \mathbf{p}\in\mathbb{R}^n\right\}.
    \end{align}

    Its subgroup $\text{SE}(n)$, the \emph{special Euclidean group}, consists of all matrices in the Euclidean group with determinant equal to 1, formed by the semidirect product $\mathbb{R}^n\rtimes \text{SO}(n)$. This group represents rigid transformations and is formed by the set of homogeneous transformation matrices:
    \begin{align}
        \text{SE}(n) = \left\{\begin{bmatrix}
            \mathbf{R} & \mathbf{p} \\ \mathbf{0} & 1
        \end{bmatrix} \in \mathbb{R}^{(n+1)\times(n+1)} | \mathbf{R}\in\text{SO}(n),\, \mathbf{p}\in\mathbb{R}^n\right\}.
    \end{align}
\end{example}
\begin{example}\label{ex:translation-group}
    The group $(\mathbb{R}^n,+)$, also denoted $\mathbb{R}^n$, is also a Lie group, where the group operation is the vector addition, the identity element is the zero vector, and the inverse map is the negation of the vector. We will usually represent this group through an inclusion map $\mathbb{R}^n \hookrightarrow \text{SE}(n)$ and denote the group under this inclusion the \emph{translation group} $\text{T}(n)$, defined as
    \begin{align}
        \text{T}(n) = \left\{\begin{bmatrix}
            \mathbf{I}_n & \mathbf{p} \\ \mathbf{0} & 1
        \end{bmatrix} \in \text{SE}(n) | \mathbf{p}\in\mathbb{R}^n\right\}.
    \end{align}
\end{example}
\begin{example}\label{ex:independent-translation-rotation-ISE}
    Although not present in the literature, we introduce the group of independent translations and rotations (\emph{Independent Special Euclidean group}) $\text{ISE}(n)\cong\mathbb{R}^n\times\text{SO}(n)$, defined as
    \begin{align}
        \begin{split}
            \text{ISE}(n) &= \left\{\begin{bmatrix}
            \mathbf{R} & \mathbf{0}\\
            \mathbf{0} & \mathbf{P}\\
            \end{bmatrix}\in\mathbb{R}^{(2n+1)\times(2n+1)}\,|\, \mathbf{R}\in\text{SO}(n),\,\mathbf{P}\in\text{T}(n)\right\}\\
            &= \left\{\begin{bmatrix}
            \mathbf{R} & \mathbf{0} & \mathbf{0}\\
            \mathbf{0} & \mathbf{I} & \mathbf{p}\\
            \mathbf{0} & \mathbf{0} & 1
            \end{bmatrix}\in\mathbb{R}^{(2n+1)\times(2n+1)}\,|\, \mathbf{R}\in\text{SO}(n),\,\mathbf{p}\in\mathbb{R}^n\right\}.
        \end{split} \label{eq:ISE-group}
    \end{align}
    Although they are isomorphic, we will use the representation $\mathbb{R}^n\times \text{SO}(n)$ to denote tuples of translations and rotations $(\mathbf{p},\mathbf{R})$, and the representation $\text{ISE}(n)$ to denote elements represented as the matrices in \eqref{eq:ISE-group}. Note that we can use both representations interchangeably, since we can easily extract the position and orientation from the matrix representation by means of matrix multiplication.
\end{example}
\begin{example}\label{ex:symplectic-group}
    The \emph{real symplectic group} $\text{Sp}(2n)$ is the group of all $2n\times 2n$ real matrices that preserve a non-degenerate skew-symmetric bilinear form $\omega$. This group plays an important role in classical mechanics, specially in the study of Hamiltonian systems. The group is defined as
    \begin{align}
        \text{Sp}(2n) &= \left\{\mathbf{X}\in\mathbb{R}^{2n\times 2n} | \mathbf{X}^\top\boldsymbol{\Omega}\mathbf{X} = \boldsymbol{\Omega}\right\}\\
        &= \left\{\mathbf{X}\in\mathbb{R}^{2n\times 2n} | -\boldsymbol{\Omega}\mathbf{X}^\top\boldsymbol{\Omega} = \mathbf{X}^{-1}\right\},
    \end{align}
    where 
    \begin{align}
        \boldsymbol{\Omega} = \begin{bmatrix}
            \mathbf{0} & \mathbf{I}_n\\
            -\mathbf{I}_n & \mathbf{0}
        \end{bmatrix}.
    \end{align}
    The skew-symmetric bilinear form can be characterized by $\omega(x,y)=\langle x, \boldsymbol{\Omega}y\rangle\,\forall\,x,y\in\mathbb{R}^{2n}$. We can also define the \emph{complex symplectic group} $\text{Sp}(2n, \mathbb{C})$ as the group of all $n\times n$ complex matrices that preserve the same non-degenerate skew-symmetric bilinear form, thus the group definition is the same, but with matrices with complex entries.
\end{example}
\begin{example}
    The \emph{indefinite orthogonal group} (or generalized orthogonal group) $\text{O}(p,q)$ is the group of all $n\times n$ real matrices
    \begin{align}
        \text{O}(p,q) = \left\{\mathbf{X}\in\mathbb{R}^{n\times n} | \mathbf{X}^\top\mathbf{I}_{p,q}\mathbf{X} = \mathbf{I}_{p,q}\right\},
    \end{align}
    where
    \begin{align}
        \mathbf{I}_{p,q} = \begin{bmatrix}
            \mathbf{I}_p & \mathbf{0}\\
            \mathbf{0} & -\mathbf{I}_q
        \end{bmatrix}.
    \end{align}
    Similarly, we can define the \emph{indefinite special orthogonal group} $\text{SO}(p,q)$ as the group
    \begin{align}
        \text{SO}(p, q) = \left\{\mathbf{X}\in\text{O}(p, q) | \det\mathbf{X}=1\right\}.
    \end{align}
    The group $\text{O}(1,3)$ is of particular interest in the study of special relativity, and is denoted the \emph{Lorentz group}. This group preserves the Lorentz metric
    \begin{align}
        (t,x,y,z) \mapsto t^2 - x^2 - y^2 - z^2.
    \end{align}
    The Lorentz group is also represented as $\text{O}(3,1)$, which would preserve the metric $x^2 + y^2 + z^2 - t^2$.
    If we restrict this group to transformations that preserve orientation, we obtain the \emph{proper Lorentz group} $\text{SO}(1,3)$. Furthermore, the subgrtoup of all Lorentz transformations that preserve orientation and the time direction is called the  \emph{proper orthochronous Lorentz group} $\text{SO}^+(1,3)$, this is the identity component $G_\iota$ of $\text{SO}(1,3)$.
    
\end{example}
% ADD Examples for indefinite orthogonal (with lorentz), heisenberg?, and symplectic groups
\begin{example}\label{ex:non-matrix-lie-group}
    An example of a Lie group that is not a matrix Lie group is given in \citet[p. 25]{Hall2015} and reproduced here:

    The group $G = \mathbb{R} \times \mathbb{R} \times \mathbb{S}^1 = \left\{(x, y, \theta) | x \in \mathbb{R}, y \in \mathbb{R}, \theta \in \mathbb{S}^1\subset\mathbb{C}\right\}$, equipped with the group operation
    \begin{align}
        (x_1, y_1, \theta_1)\circ (x_2, y_2, \theta_2) = (x_1 + x_2, y_1 + y_2, e^{ix_1y_2}\theta_1\theta_2),
    \end{align}
    is a Lie group without a matrix representation. The identity element is $\iota=(0, 0, 1)$, and the inverse map is $(x, y, \theta)^{-1} = (-x, -y, e^{ixy}\theta^{-1})$.
\end{example} 

\subsection{Lie algebras}
\begin{definition}
    A \emph{real Lie algebra} is a real vector space $\mathfrak{g}$, endowed with a map $[\cdot, \cdot]:\mathfrak{g}\times\mathfrak{g}\to\mathfrak{g}$, called the \emph{Lie bracket} on $\mathfrak{g}$, that satisfies the following properties for all $\mathbf{A},\mathbf{B},\mathbf{C}\in\mathfrak{g}$:
    \begin{property}
        \item \emph{Bilinearity}: For all $a,b\in\mathbb{R}$,
        \begin{align*}
            [a\mathbf{A}+b\mathbf{B}, \mathbf{C}] &= a[\mathbf{A}, \mathbf{C}] + b[\mathbf{B}, \mathbf{C}], \\
             [\mathbf{C}, a\mathbf{A}+b\mathbf{B}] &= a[\mathbf{C}, \mathbf{A}] + b[\mathbf{C}, \mathbf{B}];
        \end{align*}
        \item \emph{skew-symmetry}: 
        \begin{align*}
            [\mathbf{A}, \mathbf{B}] = -[\mathbf{B}, \mathbf{A}];
        \end{align*} \label{prop:lie-algebra-skew-symmetry}
        \item \emph{Jacobi identity}:
        \begin{align*}
            [\mathbf{A}, [\mathbf{B}, \mathbf{C}]] + [\mathbf{B}, [\mathbf{C}, \mathbf{A}]] + [\mathbf{C}, [\mathbf{A}, \mathbf{B}]] = 0.
        \end{align*}
    \end{property}
\end{definition}
Note that \cref{prop:lie-algebra-skew-symmetry} implies that $[\mathbf{A}, \mathbf{A}]=0\,\forall\,\mathbf{A}\in\mathfrak{g}$. 

The Lie algebra $\mathfrak{g}$ can be regarded as the tangent space at the identity $T_\iota G$

\begin{definition}
    If $\mathfrak{g}$ and $\mathfrak{h}$ are Lie algebras with Lie brackets $[\cdot, \cdot]_\mathfrak{g}$ and $[\cdot, \cdot]_\mathfrak{h}$, respectively, a linear map $\mathcal{H}:\mathfrak{g}\to\mathfrak{h}$ is denoted a \emph{homomorphism} if it preserves the Lie bracket, that is, $\mathcal{H}([\mathbf{A}, \mathbf{B}]_\mathfrak{g}) = [\mathcal{H}(\mathbf{A}), \mathcal{H}(\mathbf{B})]_\mathfrak{h}$. Furthermore, if $\mathcal{H}$ is a bijection, then $\mathcal{H}^{-1}$ is also a homomorphism, and $\mathcal{H}$ is an \emph{isomorphism}. In this case, we say that $\mathfrak{g}$ and $\mathfrak{h}$ are \emph{isomorphic} and denote this by $\mathfrak{g}\cong\mathfrak{h}$.
\end{definition}
% Its Lie algebra $\mathfrak{ise}(n)\cong\mathbb{R}^n\oplus\mathfrak{so}(n)$ and has the following representation:
%     \begin{align}
%         \mathfrak{ise}(n) &= \left\{\begin{bmatrix}
%             \mathbf{S} & \mathbf{0}\\
%             \mathbf{0} & \mathbf{U}
%         \end{bmatrix} \in \mathbb{R}^{(2n+1)\times(2n+1)}\,;\, \mathbf{S}\in\mathfrak{so}(n),\,\mathbf{U}\in\mathfrak{t}(n)\right\}\\
%         &= \left\{\begin{bmatrix}
%             \mathbf{S} & \mathbf{0} & \mathbf{0}\\
%             \mathbf{0} & \mathbf{0} & \mathbf{u}\\
%             \mathbf{0} & \mathbf{0} & 0
%         \end{bmatrix} \in \mathbb{R}^{(2n+1)\times(2n+1)}\,;\, \mathbf{S}\in\mathfrak{so}(n),\,\mathbf{u}\in\mathbb{R}^n\right\}.
%     \end{align}

\begin{theorem}[Von Neumann and Cartan, 1927]
    Let $G\subset\text{GL}(n, \mathbb{R})$ be a matrix Lie group. The set $\mathfrak{g}$ defined as
    \begin{align}
        \mathfrak{g} = \left\{\mathbf{A}\in\mathbb{R}^{n\times n} | \exp(\sigma\mathbf{A})\in G\,\forall\,\sigma\in\mathbb{R}\right\}
    \end{align}
    is a vector space equal to the tangent space $T_\iota G$ at the identity $\iota$. Furthermore, $\mathfrak{g}$ is closed under the Lie bracket $[\mathbf{A}, \mathbf{B}] = \mathbf{A}\mathbf{B} - \mathbf{B}\mathbf{A}\,\forall\,\mathbf{A},\mathbf{B}\in\mathbb{R}^{n\times n}$
\end{theorem}

\subsubsection{Start of Automatica portion}
In this section, we recall several properties of Lie groups and Lie algebras that will be essential for developing our extension. First, a Lie group $G$ is a smooth manifold equipped with a group structure. The Lie algebra $\mathfrak{g}$ associated with $G$ has two equivalent definitions, either of which we will use as appropriate. One definition describes the Lie algebra as the tangent space at the group identity $G_e$ \citep[p. 16]{Gallier2020}. Alternatively, a Lie algebra can be viewed as the space of all smooth vector fields on the Lie group, under the Lie bracket operation on vector fields \citep[p. 190]{Lee2012}. As said previously, we will consider connected matrix Lie groups, and henceforth we will denote by $n$ the dimension of any (square) matrix in a given group, and the dimension of the group by $m$, which is the same as the dimension of the associated Lie algebra.

\begin{lemma}\label{lemma:lie-group-flow}
    (Adapted from \citet[p. 570]{Gallier2020}) Given a Lie group $G$, if $V$ be a right-invariant (resp. left-invariant) vector field, $\theta:\mathbb{R}\times G$ its global flow, and $\gamma_\mathbf{X}=\theta(t, \mathbf{X})$ the associated maximal integral curve with initial condition $\mathbf{X}\in G$, then
    \begin{align*}
        \gamma_\mathbf{X}(t) = \theta(t, \mathbf{X}) &=  \theta(t, G_e)\mathbf{X}\\
        \bigl(\text{resp. }\theta(t, \mathbf{X}) &=  \mathbf{X}\theta(t, G_e)\bigr)
    \end{align*}
\end{lemma}
\begin{proof}
    Let $\gamma(t) = R_{\mathbf{X}}\theta(t, G_e)$, then $\gamma(0) = \mathbf{X}$. Applying the chain rule:
    \begin{align}
        \dot{\gamma}(t) = d(R_{\mathbf{X}})_{\theta(t, G_e)}\Bigl(V\bigl(\theta(t, G_e)\bigr)\Bigr) = V\Bigl(R_{\mathbf{X}}\bigl(\theta(t, G_e)\bigr)\Bigr) = V(\gamma(t)).
    \end{align}
    Since maximal integral curves are unique, it follows that $\gamma(t) = \theta(t, \mathbf{X})$, thus $\theta(t, \mathbf{X}) = \theta(t, G_e)\mathbf{X}$. The proof for the left-invariant case is analogous.
\end{proof}

Next, we present a fact that will be important for the forthcoming analysis. 
\begin{lemma} \label{lemma:derivative-lie-element-H-parallelizable} Let $\mathbf{G}:\mathbb{R}\to G$ be a differentiable function. Then, there exists a function $\mathbf{A}:\mathbb{R} \to \mathfrak{g}$ such that 
\begin{align}
    \frac{d}{d\sigma} \mathbf{G}(\sigma) = \mathbf{A}(\sigma) \mathbf{G}(\sigma). \label{eq:derivative-lie-element-H-parallelizable}
\end{align}

\end{lemma}
\begin{proof}
    Let $\mathbf{G}(\sigma)$ be the maximal integral curve associated with a global flow $\theta(\sigma, \mathbf{G}_0)$ of a right-invariant vector field $V$ on a Lie group $G$. By \cref{lemma:lie-group-flow}, we can write
    \begin{align}
        \frac{d}{d\sigma} \mathbf{G}(\sigma) = d(R_{\mathbf{G}(\sigma)})_{\theta(\sigma, G_e)} \Bigl(V\bigl(\theta(\sigma, G_e)\bigr)\Bigr). \label{eq:proof-derivative-lie-element-H-parallelizable-part1}
    \end{align}
    
    Since the vector space of right-invariant vector fields, denoted by $\mathfrak{g}^R$, is isomorphic (more precisely, anti-isomorphic) to the Lie algebra $\mathfrak{g}$ \citep[p. 569]{Gallier2020}, this implies that every basis for $\mathfrak{g}^R$ is a right-invariant global frame for $G$ (and consequently every Lie group is parallelizable), which in turn allows us to write $V\bigl(\theta(\sigma, G_e)\bigr) = \mathbf{A}(\sigma)$ for some $\mathbf{A}(\sigma)$ in $\mathfrak{g}$.

    Now, since in our case $R_\mathbf{X}$ is the restriction to $\text{GL}(n, \mathbb{R})$ of the linear map $\mathbf{Y}\mapsto \mathbf{Y}\mathbf{X}$, its differential can be expressed as $dR_\mathbf{X}(W) = W\mathbf{X}$ \citep[p. 194]{Lee2012}. Thus, we can write \eqref{eq:proof-derivative-lie-element-H-parallelizable-part1} as
    \begin{align}
        \frac{d}{d\sigma} \mathbf{G}(\sigma) = d(R_{\mathbf{G}(\sigma)})_{\theta(\sigma, G_e)} \bigl(\mathbf{A}(\sigma)\bigr) = \mathbf{A}(\sigma)\mathbf{G}(\sigma).
    \end{align}
    % It is a known fact that Lie groups are parallelizable using right-invariant vector fields as a basis \citep{GRIGORIAN2024804}. That means that any vector on the tangent space on any point $\mathbf{G} \in G$ can be written as $\mathbf{AG}$ in which $\mathbf{A}$ is an element of the Lie algebra $\mathfrak{g}$ (possibly different for each $\mathbf{G}$). This implies the desired result.
\end{proof}

 On the other hand, since the Lie algebra is a vector space, we can define a basis $\{\mathbf{E}_k\}\in\mathfrak{g}, k\in[1,m]$. Given a basis, for any $\mathbf{A} \in\mathfrak{g}$, there exists scalars $\{\zeta_k\},\, k\in[1,m]$, such that $\mathbf{A} = \sum_{k=1}^{m} \zeta_k\mathbf{E}_k$. Furthermore, for each choice of basis for the Lie algebra, it becomes possible to define uniquely a respective linear operator $\SL:\mathbb{R}^{m}\to\mathfrak{g}$ as follows:
\begin{definition}[S map]\label{def:SL-left-isomorphism-act-on-xi}
    Let $\mathbf{E}_1,\dots,\mathbf{E}_m$ be a basis for an $m$-dimensional Lie algebra $\mathfrak{g}$, and $\boldsymbol{\zeta}$ an $m$-dimensional vector. Then, there exists a unique isomorphism $\SL:\mathbb{R}^{m}\to\mathfrak{g}$ defined as
    \begin{equation}
        \SL[\boldsymbol{\zeta}] \triangleq \sum_{k=1}^m\zeta_k\mathbf{E}_k.    
    \end{equation}
    
\end{definition}
Note that $\SL$ is indeed a \emph{linear operator}. Some common examples of isomorphisms are as following.
\begin{example}
    In the case of the special orthogonal Lie group $\text{SO}(3)$, with a Lie algebra $\mathfrak{so}(3)$, a common basis is given by the of matrices
    \begin{align*}
        \mathbf{E}_1 = \begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & -1 \\ 0 & 1 & 0 \end{bmatrix}, \quad \mathbf{E}_2 = \begin{bmatrix} 0 & 0 & 1 \\ 0 & 0 & 0 \\ -1 & 0 & 0 \end{bmatrix}, \quad \mathbf{E}_3 = \begin{bmatrix} 0 & -1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}.
    \end{align*}
    Then, for $\boldsymbol{\zeta} = [\zeta_1, \zeta_2, \zeta_3]^\top$, the isomorphism $\SL$ is a skew-symmetric matrix given by
    \begin{align*}
        \SL[\boldsymbol{\zeta}] = \zeta_1 \mathbf{E}_1 + \zeta_2 \mathbf{E}_2 + \zeta_3 \mathbf{E}_3
        = \begin{bmatrix} 0 & -\zeta_3 & \zeta_2 \\ \zeta_3 & 0 & -\zeta_1 \\ -\zeta_2 & \zeta_1 & 0 \end{bmatrix}.
    \end{align*}
\end{example}
\begin{example}
    The full Lorentz group $\text{O}(3, 1)$, the special Lorentz group $\text{SO}(3, 1)$, and the proper orthochronous Lorentz group $\text{SO}_0(3, 1)$ share the same Lie algebra $\mathfrak{so}(3, 1)$, for which an isomosphism for a vector $\boldsymbol{\zeta} = [\zeta_1, \zeta_2, \zeta_3, \zeta_4, \zeta_5, \zeta_6]^\top$ is described as follows:
    \begin{align}
        \SL[\boldsymbol{\zeta}] = \begin{bmatrix}
            0 & \zeta_1 & \zeta_2 & \zeta_4 \\
            -\zeta_1 & 0 & \zeta_3 & \zeta_5 \\
            -\zeta_2 & -\zeta_3 & 0 & \zeta_6 \\
            \zeta_4 & \zeta_5 & \zeta_6 & 0
        \end{bmatrix}
    \end{align}
    Note that, more generally, $\mathfrak{so}(n, 1) = \left\{\begin{bmatrix} \mathbf{A} & \mathbf{a}\\
              \mathbf{a}^T & 0 \end{bmatrix} \in \mathbb{R}^{(n+1) \times (n+1)}| \mathbf{a}\in\mathbb{R}^n, \mathbf{A}^T=-\mathbf{A}\right\}$
\end{example}
\begin{example}
    The symplectic Lie algebra $\mathfrak{sp}(2n, \mathbb{R})$ is defined as
    \begin{align}
        \begin{split}
            \mathfrak{sp}(2n, \mathbb{R}) &= \left\{ \mathbf{A} \in \mathbb{R}^{2n\times 2n} | \mathbf{A}^T\boldsymbol{\Omega} + \boldsymbol{\Omega}\mathbf{A} = 0 \right\} \\&= \left\{ \begin{bmatrix} \mathbf{B} & \mathbf{C} \\ \mathbf{D} & -\mathbf{B}^T \end{bmatrix}\in \mathbb{R}^{2n\times 2n}| \mathbf{C} = \mathbf{C}^\top,\, \mathbf{D} = \mathbf{D}^\top\right\},            
        \end{split}
    \end{align}
    where $\boldsymbol{\Omega}$ the same as defined in \autoref{ex:symplectic-group}. Clearly, the dimension of $\mathfrak{sp}(2n, \mathbb{R})$ is $n(2n + 1)$. Thus,
    for a vector $\boldsymbol{\zeta} = [\zeta_1, \dots, \zeta_{2n^2+n}]^\top$, an isomorphism $\SL$ is given by
    \begin{align}
        \SL[\boldsymbol{\zeta}] = \begin{bmatrix}
            \zeta_1 & \dots & \zeta_n & \zeta_{n^2 + 1} & \dots & \zeta_{n^2 + n} \\
            \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
            \zeta_{n^2 - n + 1} & \dots & \zeta_{n^2} & \zeta_{n^2 + n} & \dots & \zeta_{\frac{3n^2+n}{2}}\\
            \zeta_{\frac{3n^2+n}{2} + 1} & \dots & \zeta_{\frac{3n^2+3n}{2}} & -\zeta_1 & \dots & -\zeta_{n^2 - n + 1}\\
            \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
            \zeta_{\frac{3n^2+3n}{2}} & \dots & \zeta_{2n^2 + n}& -\zeta_n & \dots & -\zeta_{n^2} 
        \end{bmatrix},
    \end{align}
\end{example}

Considering \autoref{lemma:derivative-lie-element-H-parallelizable} and \autoref{def:SL-left-isomorphism-act-on-xi} we can conclude the following important fact.

\begin{lemma} \label{lemma:very-important-fact}
    Given a differentiable function $\mathbf{\mathbf{G}}:\mathbb{R}\to G$, there exists a function $\boldsymbol{\zeta}:\mathbb{R}\to\mathbb{R}^m$, such that
    \begin{align}
    \label{eq:importantresult}
    \frac{d}{d\sigma} \mathbf{\mathbf{G}}(\sigma)=\SL\bigl(\boldsymbol{\zeta}(\sigma)\bigr)\mathbf{\mathbf{G}}(\sigma). 
\end{align}

\end{lemma}
\begin{proof} This is a direct consequence of \cref{lemma:derivative-lie-element-H-parallelizable} and \cref{def:SL-left-isomorphism-act-on-xi}. 
\end{proof}

Since $\SL$ is an isomorphism, the \emph{inverse map} that maps an element of the Lie algebra to a vector can be defined as well:
\begin{definition}[Inverse S map]\label{def:inverse-isomorphism-SLinv}
    Let $\mathfrak{g}$ be an $m$-dimensional Lie algebra. The \emph{inverse map} is defined as $\invSL:\mathfrak{g}\to\mathbb{R}^m$, such that $\invSL[\SL[\boldsymbol{\zeta}]] = \boldsymbol{\zeta}$. 
\end{definition}

Now, according to \cref{lemma:very-important-fact}, for each differentiable function $\mathbf{G}: \mathbb{R} \to G$ there exists a respective function $\boldsymbol{\zeta} \in \mathbb{R}^m$ according to equation \eqref{eq:importantresult}. Thus, we will define the following operator that extracts this $\boldsymbol{\zeta}(\sigma)$ from $\mathbf{G}(\sigma)$:
\begin{definition} [$\Xi$ operator] \label{def:Xioperator} Let $G$ be an $m$-dimensional Lie group. Given a choice of S map $\SL: \mathbb{R}^m \to \mathfrak{g}$, the respective $\Xi$ operator maps a differentiable function $\mathbf{G}: \mathbb{R} \to G$ into a function $\Xi[\mathbf{G}]: \mathbb{R} \to \mathbb{R}^m$ as $\Xi[\mathbf{G}](\sigma) = \SL^{-1}\bigl(\frac{d\mathbf{G}}{d\sigma}(\sigma)\mathbf{G}(\sigma)^{-1}\bigr)$. 
\end{definition}

In our development, it will be necessary to take derivatives along the manifold $G$. This is related to the concept of \emph{Lie derivatives}. For this purpose, the following definition will be useful.
\begin{definition} [\text{L} operator] \label{def:Loperator} Let $G$ be an $m$-dimensional Lie group. Given a choice of $S$ map and a differentiable function $f: G \to \mathbb{R}$, we define the \text{L} operator such that the function $\text{L}[f] : G \to \mathbb{R}^{1 \times m}$ satisfies:
\begin{equation}
\label{eq:Leq}
    \lim_{\epsilon \rightarrow 0} \frac{1}{\epsilon} \Biggl( f\Bigl(\exp\bigl(\SL[\boldsymbol{\zeta}]\epsilon\bigr)\mathbf{G}\Bigr) - f\bigl(\mathbf{G}\bigr) \Biggr) = \text{L}[f](\mathbf{G}) \boldsymbol{\zeta}
\end{equation}
for all $\boldsymbol{\zeta} \in \mathbb{R}^m$. Explicitly, the $j^{th}$ entry of the row vector $\text{L}[f](\mathbf{G})$ can be constructed as the left-hand side of \eqref{eq:Leq} when $\boldsymbol{\zeta} = \mathbf{e}_j$. In addition, if $f: G \times G \to \mathbb{R}$ is a function of two variables, $f(\mathbf{V},\mathbf{W})$, we define the \emph{partial} L operators $L_{\mathbf{V}}$ and $L_{\mathbf{W}}$ analogously as in \eqref{eq:Leq} but making the variation only in the first or in the second variable, respectively. 
\end{definition}

The following version of the chain rule using the \text{L} operator can be established.

\begin{lemma}\label{lemma:chainrule}
    Let $G$ be an $m$-dimensional Lie group. Let $\mathbf{G} : \mathbb{R} \to G$ and $f: G \to \mathbb{R}$ be differentiable functions. Then:
    \begin{equation}
       \frac{d}{d\sigma} f\bigl(\mathbf{G}(\sigma)\bigr) = \text{L}[f]\bigl(\mathbf{G}(\sigma)\bigr)\Xi[\mathbf{G}](\sigma).
    \end{equation}
\end{lemma}
\begin{proof}
    Let $\boldsymbol{\zeta}(\sigma) = \Xi[\mathbf{G}](\sigma)$, according to \cref{lemma:very-important-fact} and \cref{def:Xioperator}, we can write that for a small $\epsilon$, $\mathbf{G}(\sigma+\epsilon) \approx \exp(\SL[\boldsymbol{\zeta}(\sigma)]\epsilon)\mathbf{G}(\sigma)$. 
    Applying the definition of the traditional derivative:
    \begin{align}
        \begin{split}
            \frac{d}{d\sigma} f\bigl(\mathbf{G}(\sigma)\bigr) &= \lim_{\epsilon \rightarrow 0} \frac{f\bigl(\mathbf{G}(\sigma+\epsilon)\bigr){-}f\bigl(\mathbf{G}(\sigma)\bigr)}{\epsilon}\\
            &  =
            \lim_{\epsilon \rightarrow 0} \frac{f\bigl(\exp(\SL[\boldsymbol{\zeta}(\sigma)]\epsilon)\mathbf{G}(\sigma)\bigr){-}f\bigl(\mathbf{G}(\sigma)\bigr)}{\epsilon} = \text{L}[f](\mathbf{G}) \boldsymbol{\zeta}(\sigma)      
        \end{split}
    \end{align}
    in which the defining property of $\text{L}[f]$ in equation \eqref{eq:Leq} was applied. This concludes the proof.
\end{proof}

As a corollary of \cref{lemma:chainrule}:

\begin{corollary} \label{corol:corol1} If we have a function $f: G \times G \to \mathbb{R}$ instead of a function of a single variable, and two differentiable $\mathbf{V}, \mathbf{W} : \mathbb{R} \to G$, then:
\begin{equation}
   \frac{d}{d \sigma} f(\mathbf{V},\mathbf{W}) {=} \text{L}_{\mathbf{V}}[f] \Xi[\mathbf{V}] {+} \text{L}_{\mathbf{W}}[f] \Xi[\mathbf{W}].
\end{equation}
in which the dependency on $\mathbf{V}, \mathbf{W}$ was omitted on the right-hand side. 
\end{corollary}

\section{Adaptive Control}
Adaptive control emerges in 1950 as a response to the design of aircraft autopilots. Aircrafts operate on a wide range of speeds and altitudes, and thus its parameters suffers great variations. This inspired the main idea of adaptive control, on-line estimation of parameters based on measured system signals \citep{Slotine1991}. This need is also present for other systems: robot manipulators may need to carry objects of unknown mass; chemical processes may have unmodeled dynamics; the fuel consumption of a vehicle might pose a challenge for common controllers. In this section, we will briefly present the main concepts of adaptive control, basing our discussion on the books \citet{Slotine1991}, \citet{Krstic1995} and \citet{Ioannou2012}.

An adaptive controller is formed by the combination of a control law, designed for a nominal model, and an adaptation law, which is responsible for estimating the unkown parameters of the plant. The way the parameters estimator is designed divides the adaptive control into two main categories: \emph{direct} and \emph{indirect} adaptive control. In indirect adaptive control, the plant parameters are estimated on-line and are used to calculate the controller parameters. In the direct approach, the plant model is parameterized in terms of the controller parameters, and these are the parameters that get estimated on-line.

More specifically, in indirect approach, the plant model $P(\theta)$ is parameterized with respect to some unkown parameter vector $\theta$. For example, using the Euler-Lagrange for dynamic systems, the plant would be represented by a regressor matrix $\mathbf{Y}$ and a parameter vector $\theta$, that contains the unknowns. The adaptation law performs an on-line estimation of the parameters $\widehat{\theta}(t)$, and one obtains a estimated plant $\widehat{P}(\widehat{\theta})$. This plant treated as the true model, and is used in the design of the controller: the controller parameters $\theta_c(t)$ are obtained by solving some equation dependent on the estimated plant. A block diagram of this approach is shown in \cref{fig:indirect-adaptive-control}.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[
        block/.style={draw, rectangle, minimum height=1.2cm, minimum width=2.4cm, align=center},
        arrow/.style={->, >=stealth, thick},
        label/.style={font=\small}
    ]
    
    % Nodes
    \node[block] (controller) {Controller \\ $C(\theta_c(t))$};
    \node[block, right=2cm of controller] (plant) {Plant \\ $P(\theta)$};
    \node[block, below=1cm of plant] (estimation) {Estimation of $\theta$};
    \node[block, below=1.5cm of estimation] (calculation) {Calculations \\ $\theta_c(t) = F(\widehat{P}(\widehat{\theta}(t)))$};
    
    % Arrows between blocks
    \draw[arrow] (controller) -- node[above, label] {$u$} (plant);
    \draw[arrow] (plant) -- (estimation);
    % \draw[arrow] (plant) |- node[right, label] {$r$} (estimation);
    \draw[arrow] (estimation) -- node[right, label] {$\widehat{\theta}(t)$} (calculation);
    \draw[arrow] (calculation) -| node[left, label] {$\theta_c(t)$} (controller);
    
    % Input and Output Arrows
    \node[left=1.5cm of controller] (input) {Input $r$};
    \draw[arrow] (input) -- (controller.west);
    
    \node[right=0.75cm of estimation] (rr) {$r$};
    \draw[arrow] (rr) -- (estimation.east);

    % \node[right=0.9cm of controller] (csign) {};
    \draw[arrow] ($(controller.east)+(1,0)$) |- (estimation.west);
    
    \node[right=1.5cm of plant] (output) {$y$};
    \draw[arrow] (plant.east) -- (output);
    
    % Feedback loop
    \draw[arrow] ($(plant.east)+(0.75,0)$) |- ++(0,1.5) -| (controller.north);
    \draw[arrow] ($(plant.east)+(0.75,0)$) |- node[left, label] {} ($(estimation.east)+(0, 0.25)$);
    
    \end{tikzpicture}
    \caption{Block diagram of an indirect adaptive control system.}
    \label{fig:indirect-adaptive-control}
\end{figure}

In direct adaptive control, the plant model $P(\theta)$ is parameterized in terms of the unkown controller parameter vector $\theta_c$, such that this parametrization with the true controller parameters $P_c(\theta_c)$ renders the same results as the true plant. This parametrization is used to estimate the values of the controller parameters $\widehat{\theta}_c(t)$, which are used to calculate the control signal. A block diagram of this approach is shown in \cref{fig:direct-adaptive-control}.
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[
        block/.style={draw, rectangle, minimum height=1.2cm, minimum width=2.4cm, align=center},
        arrow/.style={->, >=stealth, thick},
        label/.style={font=\small}
    ]
    
    % Nodes
    \node[block] (controller) {Controller \\ $C(\widehat{\theta}_c)$};
    \node[block, right=2cm of controller] (plant) {Plant \\ $P(\theta)\to P_c(\theta_c)$};
    \node[block, below=1cm of plant] (estimation) {Estimation of $\theta_c$};
    
    % Arrows between blocks
    \draw[arrow] (controller) -- node[above, label] {$u$} (plant);
    \draw[arrow] (plant) -- (estimation);
    \draw[arrow] (estimation.south) |- ++(0, -0.5) -| node[left, label] {$\widehat{\theta}_c$} (controller);
    
    % Input and Output Arrows
    \node[left=1.5cm of controller] (input) {Input $r$};
    \draw[arrow] (input) -- (controller.west);
    
    \node[right=0.75cm of estimation] (rr) {$r$};
    \draw[arrow] (rr) -- (estimation.east);

    % \node[right=0.9cm of controller] (csign) {};
    \draw[arrow] ($(controller.east)+(1,0)$) |- (estimation.west);
    
    \node[right=1.5cm of plant] (output) {$y$};
    \draw[arrow] (plant.east) -- (output);
    
    % Feedback loop
    \draw[arrow] ($(plant.east)+(0.75,0)$) |- ++(0,1.5) -| (controller.north);
    \draw[arrow] ($(plant.east)+(0.75,0)$) |- node[left, label] {} ($(estimation.east)+(0, 0.25)$);
    
    \end{tikzpicture}
    \caption{Block diagram of a direct adaptive control system.}
    \label{fig:direct-adaptive-control}
\end{figure}

A common usage of the indirect approach is in Model Reference Adaptive Control (MRAC), in which the plant is compared to a reference model, and the controller is designed to make the plant behave like the reference model. This reference model is designed such that it describes the desired relationship between the input and output of the plant. The controller is then designed to change the dynamics of the plant to match the reference model. This is the approach that will be used in this text.

One common problem with adaptive control is the \emph{parameter drift}. This occurs when the input signal is not persistently exciting, that is, the input signal does not contain enough information to estimate the parameters. This can lead to the parameters drifting away from the true values, and the control system becoming unstable, even though it might initially seem stable for some interval. This is a common problem in practice, and several methods have been proposed to mitigate this issue.

The most common and simple technique is the \emph{dead-zone} or \emph{deadband}. This technique consists of setting a threshold for the parameter adaptation, such that if the tracking error is below this threshold, the adaptation dynamics are set to $0$. For example, if the adaptation law is described by $\dot{\widehat{\boldsymbol{\theta}}} = -\gamma\mathbf{Y}\mathbf{e}$, then the dead-zone implies
\begin{align*}
    \dot{\widehat{\boldsymbol{\theta}}} = \begin{cases}
        -\gamma\mathbf{Y}\mathbf{e} &\|\mathbf{e}\| > \epsilon,\\
        \mathbf{0} & \|\mathbf{e}\| \le \epsilon,
    \end{cases}
\end{align*}
where $\epsilon$ is the size of the dead-zone, or deadband.

Many common other techniques reside under the same branch of \emph{leakage} modification, which is based on Lyapunov analysis. These techniques have the same format $\dot{\widehat{\boldsymbol{\theta}}} = -\gamma\mathbf{Y}\mathbf{e} - \gamma w\widehat{\boldsymbol{\theta}}$, where $w(t)$ is the \emph{leakage term}. The most common leakage term is the \emph{$\sigma$-modification}, that consists of setting a positive small constant $\sigma$ as the leakage term. If one combines the dead-zone with the $\sigma$-modification, the result is the \emph{switching $\sigma$} technique, where $w(t)=\sigma$, with $\sigma=\sigma_0$ if $\|\widehat{\boldsymbol{\theta}}\|\ge M$, or $\sigma=\mathbf{0}$ otherwise. The \emph{$\epsilon$-modification} consists of using the norm of the tracking error in the leakage term: $w(t) = \nu_0\|\mathbf{e}\|$.

There are many other techniques to improve robustness, as covered in \citet[ch. 8]{Ioannou2012}, such as \emph{parameter projection}, which constrains the parameter estimates within predefined bounds; \emph{normalization}, which mitigates large parameter variation. With these techniques, and a choice of adaptation law, we can design an adaptive controller that is suitable for application.